# SPDX-License-Identifier: AGPL-3.0-or-later
name: Benchmark

on:
  # Disabled for PRs - use perf.yml workflow instead (realistic floors, neutral gates)
  # pull_request:
  #   branches: [ main, develop ]
  push:
    branches: [ main ]
  workflow_dispatch:

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write  # For PR comments
    steps:
    - uses: actions/checkout@v6
      with:
        fetch-depth: 0  # Full history for comparison

    - uses: dtolnay/rust-toolchain@stable
    - uses: Swatinem/rust-cache@v2
      with:
        key: benchmark

    - name: Run benchmarks
      run: |
        # Run benchmarks with performance mode enabled
        PERF=1 cargo bench --package copybook-bench -- --output-format json | tee bench_raw.json
        # Also generate human-readable output
        PERF=1 cargo bench --package copybook-bench -- --output-format pretty | tee bench.out

    - name: Process benchmark results
      id: process
      run: |
        python3 - << 'EOF'
        import json
        import glob
        import os
        import sys
        from pathlib import Path

        # Map benchmark directories to our metrics
        # Criterion stores results in target/criterion/<group_name>/<bench_name>/new/
        BENCHMARK_MAP = {
            "decode_display_heavy": "display_gibs",
            "decode_comp3_heavy": "comp3_mibs"
        }

        # Initialize results
        results = {
            "display_gibs": None,
            "comp3_mibs": None,
            "timestamp": "",
            "commit": os.environ.get("GITHUB_SHA", "unknown")[:8],
            "status": "success",
            "warnings": [],
            "errors": []
        }

        # Find criterion estimate files (these contain the final statistics)
        estimate_files = list(glob.glob('target/criterion/**/new/estimates.json', recursive=True))

        print(f"Found {len(estimate_files)} estimate files")

        # Parse criterion results
        for file in estimate_files:
            try:
                with open(file, 'r') as f:
                    data = json.load(f)

                # Extract group name from path (parent directory)
                # Path format: target/criterion/<group_name>/<bench_name>/new/estimates.json
                path_parts = Path(file).parts
                if len(path_parts) >= 4:
                    group_name = path_parts[-4]  # Get the group name
                    bench_name = path_parts[-3]  # Get the benchmark name

                    print(f"Processing {group_name}/{bench_name}")

                    if group_name in BENCHMARK_MAP:
                        metric_name = BENCHMARK_MAP[group_name]

                        # Extract the mean throughput estimate
                        if 'mean' in data and 'point_estimate' in data['mean']:
                            # Criterion stores throughput in elements/second or bytes/second
                            # We need to find the benchmark.json to get the throughput info
                            benchmark_json = file.replace('estimates.json', 'benchmark.json')

                            if os.path.exists(benchmark_json):
                                with open(benchmark_json, 'r') as bf:
                                    bench_data = json.load(bf)

                                if 'throughput' in bench_data:
                                    throughput_info = bench_data['throughput']
                                    mean_time_ns = data['mean']['point_estimate']

                                    if throughput_info.get('Bytes') is not None:
                                        # Calculate bytes per second from time per iteration
                                        bytes_per_iter = throughput_info['Bytes']
                                        time_per_iter_sec = mean_time_ns / 1e9
                                        bytes_per_sec = bytes_per_iter / time_per_iter_sec

                                        if metric_name == "display_gibs":
                                            gibs = bytes_per_sec / (1024**3)
                                            results["display_gibs"] = max(results["display_gibs"] or 0, round(gibs, 2))
                                            print(f"  DISPLAY throughput: {gibs:.2f} GiB/s")
                                        elif metric_name == "comp3_mibs":
                                            mibs = bytes_per_sec / (1024**2)
                                            results["comp3_mibs"] = max(results["comp3_mibs"] or 0, round(mibs, 1))
                                            print(f"  COMP-3 throughput: {mibs:.1f} MiB/s")

            except Exception as e:
                error_msg = f"Failed to parse {file}: {e}"
                results["errors"].append(error_msg)
                print(f"Error: {error_msg}")

        # Validate we got results
        if results["display_gibs"] is None and results["comp3_mibs"] is None:
            results["errors"].append("No throughput data found in benchmark results")
            results["status"] = "failure"

        # Validate against SLOs
        # NOTE: These aspirational targets (4.1 GiB/s DISPLAY, 560 MiB/s COMP-3) are historic
        # and not aligned with established baseline (205 MiB/s DISPLAY, 58 MiB/s COMP-3).
        # See Issues #89, #95 for context. Current policy (refs #74, #75): accuracy-first,
        # perf gates are NEUTRAL. Active workflow perf.yml uses realistic floors
        # (DISPLAY ‚â• 80 MiB/s, COMP-3 ‚â• 40 MiB/s) with neutral/advisory status.
        slo_display_gibs = 4.1  # Historic target, not current baseline
        slo_comp3_mibs = 560    # Historic target, not current baseline

        if results["display_gibs"] is not None:
            if results["display_gibs"] < slo_display_gibs:
                results["status"] = "failure"
                results["errors"].append(f"DISPLAY throughput {results['display_gibs']} GiB/s below SLO {slo_display_gibs} GiB/s")
            elif results["display_gibs"] < slo_display_gibs * 1.05:  # Within 5% warning threshold
                results["warnings"].append(f"DISPLAY throughput {results['display_gibs']} GiB/s close to SLO threshold")

        if results["comp3_mibs"] is not None:
            if results["comp3_mibs"] < slo_comp3_mibs:
                results["status"] = "failure"
                results["errors"].append(f"COMP-3 throughput {results['comp3_mibs']} MiB/s below SLO {slo_comp3_mibs} MiB/s")
            elif results["comp3_mibs"] < slo_comp3_mibs * 1.05:  # Within 5% warning threshold
                results["warnings"].append(f"COMP-3 throughput {results['comp3_mibs']} MiB/s close to SLO threshold")

        # Write results
        Path('perf.json').write_text(json.dumps(results, indent=2))

        # Set outputs for GitHub Actions
        print(f"::set-output name=status::{results['status']}")
        print(f"::set-output name=display_gibs::{results['display_gibs'] or 'N/A'}")
        print(f"::set-output name=comp3_mibs::{results['comp3_mibs'] or 'N/A'}")

        # Report status (advisory only per #74, #75)
        if results["status"] == "failure":
            print("‚ö†Ô∏è Historic benchmark SLOs not met (advisory only)!")
            for error in results["errors"]:
                print(f"   {error}")
            print("   See perf.yml for neutral gates with realistic floors")
        elif results["warnings"]:
            print("‚ö†Ô∏è  Benchmark warnings:")
            for warning in results["warnings"]:
                print(f"   {warning}")
        else:
            print("‚úÖ All benchmark SLOs passed!")
            print(f"   DISPLAY: {results['display_gibs']} GiB/s")
            print(f"   COMP-3: {results['comp3_mibs']} MiB/s")
        EOF

    - name: Upload benchmark artifacts
      uses: actions/upload-artifact@v6
      if: always()
      with:
        name: benchmark-results-${{ github.sha }}
        path: |
          perf.json
          bench.out
          bench_raw.json
          target/criterion/**/*.json
          target/criterion/**/*.html
        retention-days: 14
        compression-level: 6

    - name: Comment PR with benchmark results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v8
      with:
        script: |
          const fs = require('fs');

          try {
            const perfData = JSON.parse(fs.readFileSync('perf.json', 'utf8'));

            const statusIcon = perfData.status === 'failure' ? '‚ùå' :
                             perfData.warnings.length > 0 ? '‚ö†Ô∏è' : '‚úÖ';

            const displayMetric = perfData.display_gibs ? `${perfData.display_gibs} GiB/s` : 'N/A';
            const comp3Metric = perfData.comp3_mibs ? `${perfData.comp3_mibs} MiB/s` : 'N/A';

            let body = `## ${statusIcon} Benchmark Results\n\n`;
            body += `**Commit:** \`${perfData.commit}\`\n\n`;
            body += `| Metric | Result | SLO | Status |\n`;
            body += `|--------|--------|-----|--------|\n`;
            body += `| DISPLAY | ${displayMetric} | ‚â•4.1 GiB/s | ${perfData.display_gibs >= 4.1 ? '‚úÖ' : '‚ùå'} |\n`;
            body += `| COMP-3 | ${comp3Metric} | ‚â•560 MiB/s | ${perfData.comp3_mibs >= 560 ? '‚úÖ' : '‚ùå'} |\n\n`;

            if (perfData.errors.length > 0) {
              body += `### ‚ùå Errors\n`;
              perfData.errors.forEach(error => {
                body += `- ${error}\n`;
              });
              body += `\n`;
            }

            if (perfData.warnings.length > 0) {
              body += `### ‚ö†Ô∏è Warnings\n`;
              perfData.warnings.forEach(warning => {
                body += `- ${warning}\n`;
              });
              body += `\n`;
            }

            body += `### üìä Performance Specifications\n`;
            body += `For detailed performance analysis, see [Performance Specifications](https://github.com/EffortlessMetrics/copybook-rs#performance-specifications) in the main README.\n\n`;
            body += `**Benchmark artifacts available for 14 days.**`;

            // Find existing benchmark comment to update
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number
            });

            const existingComment = comments.find(comment =>
              comment.body.includes('## üìä Benchmark Results') ||
              comment.body.includes('## ‚úÖ Benchmark Results') ||
              comment.body.includes('## ‚ùå Benchmark Results') ||
              comment.body.includes('## ‚ö†Ô∏è Benchmark Results')
            );

            if (existingComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }
          } catch (error) {
            console.error('Failed to post benchmark comment:', error);
          }

    - name: Compare against baseline (AC5)
      id: baseline_check
      if: github.event_name == 'pull_request'
      run: |
        # Download existing baseline if available
        gh api repos/${{ github.repository }}/actions/artifacts \
          --jq '.artifacts[] | select(.name | startswith("baseline-main")) | select(.created_at | . != null) | .archive_download_url' \
          --paginate | head -1 | xargs -I {} sh -c '
          if [ -n "{}" ]; then
            echo "Downloading latest main baseline..."
            gh api {} > baseline.zip 2>/dev/null || true
            if [ -s baseline.zip ]; then
              unzip -q baseline.zip 2>/dev/null || true
              if [ -f performance.json ]; then
                echo "Found baseline, running comparison..."
                cargo run --bin bench-report -p copybook-bench -- compare perf.json
              else
                echo "No valid baseline found in artifact"
              fi
            else
              echo "Failed to download baseline"
            fi
          else
            echo "No baseline artifact found"
          fi
        ' || echo "Baseline comparison skipped (no baseline available)"
      env:
        GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

    - name: Promote baseline (AC5)
      if: github.ref == 'refs/heads/main' && steps.process.outputs.status == 'success'
      run: |
        # Promote current results to baseline
        cargo run --bin bench-report -p copybook-bench -- baseline promote perf.json
        echo "‚úÖ Promoted performance baseline for main branch"

    - name: Upload baseline for main branch
      if: github.ref == 'refs/heads/main' && steps.process.outputs.status == 'success'
      uses: actions/upload-artifact@v6
      with:
        name: baseline-main-${{ github.sha }}
        path: target/baselines/performance.json
        retention-days: 90

    - name: Advisory SLO status (neutral gate)
      if: steps.process.outputs.status == 'failure'
      run: |
        echo "‚ö†Ô∏è Historic SLO targets not met (advisory only)"
        echo "NOTE: This workflow uses historic aspirational targets (4.1 GiB/s DISPLAY, 560 MiB/s COMP-3)."
        echo "Current policy (refs #74, #75): perf gates are NEUTRAL/ADVISORY (accuracy-first)."
        echo "Active workflow perf.yml uses realistic floors with neutral status."
        echo "See tools/copybook-bench/BASELINE_METHODOLOGY.md for established baseline (205 MiB/s DISPLAY, 58 MiB/s COMP-3)."
        echo "Legacy SLOs are advisory; see perf.yml (neutral)" && exit 0