Feature: CLI Exit Codes
  Scenario: Success returns exit code 0
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "HELLOWORLD"
   ✔  When the binary data is decoded
   ✔  When the exit code is computed from the error
   ✔  Then the exit code should be 0
  Scenario: Parse error returns non-zero exit code
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✔  When the exit code is computed from the error
   ✔  Then the exit code should be 1
  Scenario: Exit code 0 means success
   ✔  Then exit code 0 means success
  Scenario: Valid decode has exit code 0
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "HELLO"
   ✔  When the binary data is decoded
   ✔  When the exit code is computed from the error
   ✔  Then the exit code should be 0
  Scenario: Valid encode has exit code 0
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And JSON data: "{\"FIELD-A\":\"HELLO\"}"
   ✔  When the JSON data is encoded
   ✔  When the exit code is computed from the error
   ✔  Then the exit code should be 0
  Scenario: Successful round-trip has exit code 0
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "HELLOWORLD"
   ✔  When the data is round-tripped
   ✔  When the exit code is computed from the error
   ✔  Then the exit code should be 0
  Scenario: Successful parse has exit code 0
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then parsing should succeed
   ✔  When the exit code is computed from the error
   ✔  Then the exit code should be 0
  Scenario: Valid copybook with numeric fields exits 0
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "12345"
   ✔  When the binary data is decoded
   ✔  When the exit code is computed from the error
   ✔  Then the exit code should be 0
  Scenario: Valid COMP-3 decode exits 0
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "\x01\x23\x4C"
   ✔  When the binary data is decoded
   ✔  When the exit code is computed from the error
   ✔  Then the exit code should be 0
  Scenario: Valid binary integer decode exits 0
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "\x00\x2A"
   ✔  When the binary data is decoded
   ✔  When the exit code is computed from the error
   ✔  Then the exit code should be 0
Feature: Codec Edge Cases
  Scenario: Decoding empty record (zero-length binary)
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: ""
   ✔  When the binary data is decoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\codec_edge_cases.feature:21:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: Decoding record shorter than schema expects
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "SHORT"
   ✔  When the binary data is decoded
   ✔  Then an error should occur
  Scenario: Decoding record longer than schema with extra trailing data
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "HELLOEXTRA_TRAILING_BYTES"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
   ✔  And decoded field FIELD-A should be "HELLO"
  Scenario: COMP-3 packed decimal with invalid nibbles
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "\xFF\xFF\xFF\xFF\xFF"
   ✔  When the binary data is decoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\codec_edge_cases.feature:61:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: COMP-3 with positive sign nibble C
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "\x12\x3C"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: COMP-3 with negative sign nibble D
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "\x12\x3D"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: COMP-3 with unsigned sign nibble F
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "\x12\x3F"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: DISPLAY numeric with leading spaces
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "  123"
   ✔  When the binary data is decoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\codec_edge_cases.feature:108:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: DISPLAY numeric with trailing spaces
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "123  "
   ✔  When the binary data is decoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\codec_edge_cases.feature:118:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: PIC X field with all-spaces content
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "          "
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: PIC X field with binary zeros
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "\x00\x00\x00\x00"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: PIC 9 field containing alphabetic characters
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "ABCDE"
   ✔  When the binary data is decoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\codec_edge_cases.feature:154:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: PIC 9 field containing mixed alpha-numeric
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "12A45"
   ✔  When the binary data is decoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\codec_edge_cases.feature:164:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: Signed zoned decimal with positive overpunch
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "1234{"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Signed zoned decimal with negative overpunch
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "1234}"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: COMP binary field with zero value
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "\x00\x00"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: COMP binary field with max positive halfword
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "\x7F\xFF"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: COMP binary field with min negative halfword
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "\x80\x00"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: COMP binary fullword with zero
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "\x00\x00\x00\x00"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: COMP binary fullword with max positive value
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "\x7F\xFF\xFF\xFF"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: COMP binary fullword with min negative value
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "\x80\x00\x00\x00"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
Feature: Codepage Variants
  Scenario: Decode with CP037 codepage
   ✔> Given a copybook with content:
   ✔  Given codepage "CP037"
   ✔  And binary data for all fields
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Decode with CP273 codepage
   ✔> Given a copybook with content:
   ✔  Given codepage "CP273"
   ✔  And binary data for all fields
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Decode with CP500 codepage
   ✔> Given a copybook with content:
   ✔  Given codepage "CP500"
   ✔  And binary data for all fields
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Decode with CP1047 codepage
   ✔> Given a copybook with content:
   ✔  Given codepage "CP1047"
   ✔  And binary data for all fields
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Decode with CP1140 codepage
   ✔> Given a copybook with content:
   ✔  Given codepage "CP1140"
   ✔  And binary data for all fields
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Encode with CP037 codepage
   ✔> Given a copybook with content:
   ✔  Given codepage "CP037"
   ✔  And JSON data: "{\"TEXT-FIELD\":\"HELLO\",\"NUM-FIELD\":\"12345\"}"
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
  Scenario: Encode with CP273 codepage
   ✔> Given a copybook with content:
   ✔  Given codepage "CP273"
   ✔  And JSON data: "{\"TEXT-FIELD\":\"HELLO\",\"NUM-FIELD\":\"12345\"}"
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
  Scenario: Encode with CP500 codepage
   ✔> Given a copybook with content:
   ✔  Given codepage "CP500"
   ✔  And JSON data: "{\"TEXT-FIELD\":\"HELLO\",\"NUM-FIELD\":\"12345\"}"
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
  Scenario: Encode with CP1047 codepage
   ✔> Given a copybook with content:
   ✔  Given codepage "CP1047"
   ✔  And JSON data: "{\"TEXT-FIELD\":\"HELLO\",\"NUM-FIELD\":\"12345\"}"
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
  Scenario: Encode with CP1140 codepage
   ✔> Given a copybook with content:
   ✔  Given codepage "CP1140"
   ✔  And JSON data: "{\"TEXT-FIELD\":\"HELLO\",\"NUM-FIELD\":\"12345\"}"
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
  Scenario: Round-trip with CP037
   ✔> Given a copybook with content:
   ✔  Given codepage "CP037"
   ✔  And binary data for all fields
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
  Scenario: Round-trip with CP500
   ✔> Given a copybook with content:
   ✔  Given codepage "CP500"
   ✔  And binary data for all fields
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
  Scenario: Round-trip with CP1047
   ✔> Given a copybook with content:
   ✔  Given codepage "CP1047"
   ✔  And binary data for all fields
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
  Scenario: Round-trip with CP1140
   ✔> Given a copybook with content:
   ✔  Given codepage "CP1140"
   ✔  And binary data for all fields
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
  Scenario: Decode COMP-3 with CP037 codepage
   ✔> Given a copybook with content:
   ✔  Given a copybook with content:
   ✔  And codepage "CP037"
   ✔  And binary data: "\x00\x00\x12\x34\x5C"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: ASCII codepage decode
   ✔> Given a copybook with content:
   ✔  Given ASCII codepage
   ✔  And binary data for all fields
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: ASCII codepage encode
   ✔> Given a copybook with content:
   ✔  Given ASCII codepage
   ✔  And JSON data: "{\"TEXT-FIELD\":\"HELLO\",\"NUM-FIELD\":\"12345\"}"
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
  Scenario: ASCII round-trip is lossless
   ✔> Given a copybook with content:
   ✔  Given ASCII codepage
   ✔  And binary data for all fields
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
Feature: Copybook Parsing
  Scenario: Parse a simple copybook with a single field
   ✔  Given a simple copybook with a single field
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ?  And the schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\copybook_parsing.feature:12:5
  Scenario: Parse a copybook with numeric fields
   ✔  Given a copybook with numeric fields
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ?  And the schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\copybook_parsing.feature:20:5
  Scenario: Parse a copybook with OCCURS clause
   ✔  Given a copybook with OCCURS clause
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ?  And the schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\copybook_parsing.feature:29:5
  Scenario: Parse a copybook with ODO (OCCURS DEPENDING ON)
   ✔  Given a copybook with ODO (OCCURS DEPENDING ON)
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ?  And the schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\copybook_parsing.feature:36:5
  Scenario: Parse a copybook with REDEFINES clause
   ✔  Given a copybook with REDEFINES clause
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ?  And the schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\copybook_parsing.feature:44:5
  Scenario: Parse a copybook with Level-88 condition values
   ✔  Given a copybook with Level-88 condition values
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ?  And the schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\copybook_parsing.feature:52:5
  Scenario: Parse copybook in strict mode
   ✔  Given strict parsing mode
   ✔  And a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
  Scenario: Parse copybook in tolerant mode
   ✔  Given tolerant parsing mode
   ✔  And a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
  Scenario: Parse copybook with inline comments
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ?  And the schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\copybook_parsing.feature:85:5
  Scenario: Verify field offsets are calculated correctly
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ✔  And the field "FIELD-1" should have offset 0
   ✔  And the field "FIELD-1" should have length 5
   ✔  And the field "FIELD-2" should have offset 5
   ✔  And the field "FIELD-2" should have length 5
   ✔  And the field "FIELD-3" should have offset 10
   ✔  And the field "FIELD-3" should have length 10
  Scenario: Parse copybook with nested group structures
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ?  And the schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\copybook_parsing.feature:116:5
  Scenario: Parse level-88 VALUE list commas without treating them as edited picture tokens
   ✔  Given a copybook with an inline VALUE-list for Level-88
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ?  And the schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\copybook_parsing.feature:124:5
Feature: Decode Edge Cases
  Scenario: Decode alphanumeric field with all spaces
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "          "
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
   ✔  And the decoded output should contain "NAME-FIELD"
  Scenario: Decode multiple fields where some are all spaces
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "HELLO     00042"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
   ✔  And decoded field FILLED-FIELD should be "HELLO"
  Scenario: Decode record that is too short for schema
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "SHORT"
   ✔  When the binary data is decoded
   ✔  Then an error should occur
  Scenario: Decode empty binary data
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: ""
   ✔  When the binary data is decoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\decode_edge_cases.feature:60:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: Decode COMP-3 field with zero value
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "\x00\x00\x00\x00\x0C"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Decode COMP-3 field with positive value
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "\x01\x23\x45\x67\x8C"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Decode COMP-3 field with negative value
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "\x01\x23\x45\x67\x8D"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Decode COMP-3 field with invalid nibble
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "\xFF\xFF\xFF\xFF\xFF"
   ✔  When the binary data is decoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\decode_edge_cases.feature:109:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: Decode with CP037 EBCDIC codepage
   ✔  Given a copybook with content:
   ✔  And codepage "CP037"
   ✔  And binary data: "\xC1\xC2\xC3\xC4\xC5"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
   ✔  And the decoded output should contain "NAME-FIELD"
  Scenario: Decode with CP500 EBCDIC codepage
   ✔  Given a copybook with content:
   ✔  And codepage "CP500"
   ✔  And binary data: "\xC1\xC2\xC3\xC4\xC5"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Decode with CP1047 EBCDIC codepage
   ✔  Given a copybook with content:
   ✔  And codepage "CP1047"
   ✔  And binary data: "\xC1\xC2\xC3\xC4\xC5"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Decode zoned decimal field with all zeros
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "00000"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Decode zoned decimal field with max value
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "99999"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Decode COMP binary integer with zero
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "\x00\x00"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
Feature: Determinism Validation
  Scenario: Decode determinism with identical output
   ✔  Given a simple copybook with a single field
   ✔  And ASCII codepage
   ✔  And binary data: "ABCDEFGHIJ"
   ✔  When decode determinism is checked
   ✔  Then the decode should be deterministic
   ✔  And the round 1 hash should equal to round 2 hash
   ✔  And there should be no byte differences
  Scenario: Encode determinism with identical output
   ✔  Given a simple copybook with a single field
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When encode determinism is checked
   ✔  Then the encode should be deterministic
   ✔  And the round 1 hash should equal to round 2 hash
   ✔  And there should be no byte differences
  Scenario: Round-trip determinism with identical output
   ✔  Given a simple copybook with a single field
   ✔  And ASCII codepage
   ✔  And binary data: "ABCDEFGHIJ"
   ✔  When round-trip determinism is checked
   ✔  Then the round-trip should be deterministic
   ✔  And the round 1 hash should equal to round 2 hash
   ✔  And there should be no byte differences
  Scenario: Decode determinism with multiple fields
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "ABCDE12345"
   ✔  When decode determinism is checked
   ✔  Then the decode should be deterministic
   ✔  And the round 1 hash should equal to round 2 hash
  Scenario: Encode determinism with multiple fields
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When encode determinism is checked
   ✔  Then the encode should be deterministic
   ✔  And the round 1 hash should equal to round 2 hash
  Scenario: Determinism result generates JSON output for CI integration
   ✔  Given a simple copybook with a single field
   ✔  And ASCII codepage
   ✔  And binary data: "ABCDEFGHIJ"
   ✔  When decode determinism is checked
   ✔  Then the JSON should contain "mode"
   ✔  And the JSON should contain "round1_hash"
   ✔  And the JSON should contain "round2_hash"
   ✔  And the JSON should contain "is_deterministic"
  Scenario: Human-readable determinism output shows verdict
   ✔  Given a simple copybook with a single field
   ✔  And ASCII codepage
   ✔  And binary data: "ABCDEFGHIJ"
   ✔  When decode determinism is checked
   ✔  Then the human-readable output should show "DETERMINISTIC"
   ✔  And the output should contain "Round 1 hash"
   ✔  And the output should contain "Round 2 hash"
  Scenario: Determinism hashes use canonical BLAKE3 hex shape
   ✔  Given a simple copybook with a single field
   ✔  And ASCII codepage
   ✔  And binary data: "ABCDEFGHIJ"
   ✔  When decode determinism is checked
   ✔  Then the round 1 hash should be canonical blake3 hex
   ✔  And the round 2 hash should be canonical blake3 hex
Feature: Dialect Lever for ODO (OCCURS DEPENDING ON) Behavior
  Scenario: Parse copybook in Normative dialect (default)
   ✔> Given a copybook with ODO (OCCURS DEPENDING ON)
   ✔> And ASCII codepage
   ✔  Given a copybook with content:
   ✔  And Normative dialect
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ✔  And the field "COUNT-FIELD" should have type "numeric"
   ✔  And the field "DYNAMIC-ARRAY" should have type "occurs"
  Scenario: Parse copybook in Zero-Tolerant dialect
   ✔> Given a copybook with ODO (OCCURS DEPENDING ON)
   ✔> And ASCII codepage
   ✔  Given a copybook with content:
   ✔  And Zero-Tolerant dialect
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ✔  And the field "COUNT-FIELD" should have type "numeric"
   ✔  And the field "DYNAMIC-ARRAY" should have type "occurs"
  Scenario: Parse copybook in One-Tolerant dialect
   ✔> Given a copybook with ODO (OCCURS DEPENDING ON)
   ✔> And ASCII codepage
   ✔  Given a copybook with content:
   ✔  And One-Tolerant dialect
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ✔  And the field "COUNT-FIELD" should have type "numeric"
   ✔  And the field "DYNAMIC-ARRAY" should have type "occurs"
  Scenario: Decode ODO with Normative min_count enforcement (valid count)
   ✔> Given a copybook with ODO (OCCURS DEPENDING ON)
   ✔> And ASCII codepage
   ✔  Given a copybook with content:
   ✔  And Normative dialect
   ✔  And binary data: "005ELEMENT001ELEMENT002ELEMENT003ELEMENT004ELEMENT005"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should contain "ELEMENT001"
   ✘  And the decoded output should contain "ELEMENT005"
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\dialect_lever.feature:66:5
      Matched: tests\bdd\steps\encode_decode.rs:424:1
      Step panicked. Captured output: Expected decoded output to contain 'ELEMENT005', got: {"schema":"copybook.v1","record_index":1,"codepage":"ascii","fields":{"COUNT-FIELD":"005","DYNAMIC-ARRAY":[{"ELEMENT":"ELEMENT001"},{"ELEMENT":"ELEMENT001"},{"ELEMENT":"ELEMENT001"},{"ELEMENT":"ELEMENT001"},{"ELEMENT":"ELEMENT001"}]},"COUNT-FIELD":"005","DYNAMIC-ARRAY":[{"ELEMENT":"ELEMENT001"},{"ELEMENT":"ELEMENT001"},{"ELEMENT":"ELEMENT001"},{"ELEMENT":"ELEMENT001"},{"ELEMENT":"ELEMENT001"}],"schema_fingerprint":"a7668e965ead53653b757a302a814dd0d5d8389aa727ce52aa504bb1776dca97","__schema_id":"a7668e965ead53653b757a302a814dd0d5d8389aa727ce52aa504bb1776dca97","length":53,"__record_index":1,"__length":53}
  Scenario: Decode ODO with Normative min_count enforcement (invalid count)
   ✔> Given a copybook with ODO (OCCURS DEPENDING ON)
   ✔> And ASCII codepage
   ✔  Given a copybook with content:
   ✔  And Normative dialect
   ✔  And binary data: "003ELEMENT001ELEMENT002ELEMENT003"
   ✔  When the binary data is decoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\dialect_lever.feature:79:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: Decode ODO with Zero-Tolerant min_count ignored (zero count)
   ✔> Given a copybook with ODO (OCCURS DEPENDING ON)
   ✔> And ASCII codepage
   ✔  Given a copybook with content:
   ✔  And Zero-Tolerant dialect
   ✔  And binary data: "000"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Decode ODO with Zero-Tolerant min_count ignored (below min_count)
   ✔> Given a copybook with ODO (OCCURS DEPENDING ON)
   ✔> And ASCII codepage
   ✔  Given a copybook with content:
   ✔  And Zero-Tolerant dialect
   ✔  And binary data: "003ELEMENT001ELEMENT002ELEMENT003"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✘  And the decoded output should contain "ELEMENT001"
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\dialect_lever.feature:108:5
      Matched: tests\bdd\steps\encode_decode.rs:424:1
      Step panicked. Captured output: Expected decoded output to contain 'ELEMENT001', got: 
  Scenario: Decode ODO with One-Tolerant min_count clamped (zero min_count)
   ✔> Given a copybook with ODO (OCCURS DEPENDING ON)
   ✔> And ASCII codepage
   ✔  Given a copybook with content:
   ✔  And One-Tolerant dialect
   ✔  And binary data: "000"
   ✔  When the binary data is decoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\dialect_lever.feature:121:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: Decode ODO with One-Tolerant min_count clamped (valid count)
   ✔> Given a copybook with ODO (OCCURS DEPENDING ON)
   ✔> And ASCII codepage
   ✔  Given a copybook with content:
   ✔  And One-Tolerant dialect
   ✔  And binary data: "001ELEMENT001"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should contain "ELEMENT001"
  Scenario: Parse copybook with complex ODO structure in different dialects
   ✔> Given a copybook with ODO (OCCURS DEPENDING ON)
   ✔> And ASCII codepage
   ✔  Given a copybook with content:
   ✔  And Normative dialect
   ✔  When the copybook is parsed
   ✘  Then the schema should be successfully parsed
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\dialect_lever.feature:153:5
      Matched: tests\bdd\steps\parsing.rs:138:1
      Step panicked. Captured output: Schema parsing failed with error: CBKP021_ODO_NOT_TAIL: ODO array 'COMPLEX-ODO.HEADERS' must be last storage field under 'COMPLEX-ODO'
  Scenario: CLI --dialect flag overrides environment variable
   ✔> Given a copybook with ODO (OCCURS DEPENDING ON)
   ✔> And ASCII codepage
   ✔  Given a copybook with content:
   ✔  And Zero-Tolerant dialect
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ✔  And the field "DYNAMIC-ARRAY" should have type "occurs"
  Scenario: Error handling for invalid dialect modes
   ✔> Given a copybook with ODO (OCCURS DEPENDING ON)
   ✔> And ASCII codepage
   ✔  Given a copybook with content:
   ✔  And invalid dialect mode
   ✔  When the copybook is parsed
   ✔  Then an error should occur
   ✔  And the error message should contain "dialect"
  Scenario: Integration with existing copybook parsing scenarios
   ✔> Given a copybook with ODO (OCCURS DEPENDING ON)
   ✔> And ASCII codepage
   ✔  Given a copybook with content:
   ✔  And One-Tolerant dialect
   ✔  When the copybook is parsed
   ✘  Then the schema should be successfully parsed
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\dialect_lever.feature:195:5
      Matched: tests\bdd\steps\parsing.rs:138:1
      Step panicked. Captured output: Schema parsing failed with error: CBKP021_ODO_NOT_TAIL: ODO array 'INTEGRATION-TEST.DYNAMIC-ARRAY' must be last storage field under 'INTEGRATION-TEST'
  Scenario: Round-trip with ODO in different dialects
   ✔> Given a copybook with ODO (OCCURS DEPENDING ON)
   ✔> And ASCII codepage
   ✔  Given a copybook with content:
   ✔  And Zero-Tolerant dialect
   ✔  And binary data: "002ELEM1ELEM2"
   ✔  When the data is round-tripped
   ✘  Then the round-trip should be lossless
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\dialect_lever.feature:212:5
      Matched: tests\bdd\steps\encode_decode.rs:511:1
      Step panicked. Captured output: assertion `left == right` failed: Round-trip should be lossless: original data differs from encoded data
        left: [48, 48, 50, 69, 76, 69, 77, 49, 69, 76, 69, 77, 50]
       right: []
Feature: Dialect Modes for ODO Behavior
  Scenario: Normative dialect enforces declared min_count
   ✔  Given a copybook with content:
   ✔  And Normative dialect
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ✔  And field ITEMS should have ODO with min 5 and max 20
  Scenario: Normative dialect rejects count below declared min
   ✔  Given a copybook with content:
   ✔  And Normative dialect
   ✔  And ASCII codepage
   ✔  And binary data: "002AAAABBBB"
   ✔  When the binary data is decoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\dialect_modes.feature:34:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: Normative dialect accepts count at declared min
   ✔  Given a copybook with content:
   ✔  And Normative dialect
   ✔  And ASCII codepage
   ✔  And binary data: "005AAAABBBBCCCCDDDDEEEE"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Zero-tolerant dialect allows zero count
   ✔  Given a copybook with content:
   ✔  And Zero-Tolerant dialect
   ✔  And ASCII codepage
   ✔  And binary data: "000"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Zero-tolerant dialect allows count below declared min
   ✔  Given a copybook with content:
   ✔  And Zero-Tolerant dialect
   ✔  And ASCII codepage
   ✔  And binary data: "002AAAABBBB"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Zero-tolerant dialect still respects max count
   ✔  Given a copybook with content:
   ✔  And Zero-Tolerant dialect
   ✔  And ASCII codepage
   ✔  And binary data: "005AAAABBBBCCCCDDDDEEEE"
   ✔  When the binary data is decoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\dialect_modes.feature:95:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: One-tolerant dialect clamps min_count to 1
   ✔  Given a copybook with content:
   ✔  And One-Tolerant dialect
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ✘  And field ITEMS should have ODO with min 1 and max 20
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\dialect_modes.feature:110:5
      Matched: tests\bdd\steps\parsing.rs:325:1
      Step panicked. Captured output: assertion `left == right` failed: Field 'ITEMS' ODO min should be 1
        left: 0
       right: 1
  Scenario: One-tolerant dialect rejects zero count when min declared as 0
   ✔  Given a copybook with content:
   ✔  And One-Tolerant dialect
   ✔  And ASCII codepage
   ✔  And binary data: "000"
   ✔  When the binary data is decoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\dialect_modes.feature:124:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: One-tolerant dialect accepts count of 1
   ✔  Given a copybook with content:
   ✔  And One-Tolerant dialect
   ✔  And ASCII codepage
   ✔  And binary data: "001AAAA"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Dialect setting does not affect simple OCCURS
   ✔  Given a copybook with content:
   ✔  And Zero-Tolerant dialect
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ✔  And field ITEMS should have OCCURS count 3
  Scenario: Dialect setting does not affect alphanumeric fields
   ✔  Given a copybook with content:
   ✔  And One-Tolerant dialect
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ✔  And the field "NAME-FIELD" should have type "alphanumeric"
Feature: Edited PIC BDD Coverage
  Scenario: Decode zero-suppressed field PIC ZZZ9
   ✔> Given ASCII codepage
   ✔  Given a copybook with edited PIC:
   ✔  And binary data: " 123"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Decode zero-suppressed field with all leading spaces
   ✔> Given ASCII codepage
   ✔  Given a copybook with edited PIC:
   ✔  And binary data: "   0"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Decode zero-suppressed field with decimal PIC ZZZ9.99
   ✔> Given ASCII codepage
   ✔  Given a copybook with edited PIC:
   ✔  And binary data: "  12.34"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Decode currency-edited field PIC $ZZ,ZZZ.99
   ✔> Given ASCII codepage
   ✔  Given a copybook with edited PIC:
   ✔  And binary data: "$ 1,234.56"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Decode currency-edited field with small value
   ✔> Given ASCII codepage
   ✔  Given a copybook with edited PIC:
   ✔  And binary data: "$     5.00"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Decode check-protect field PIC ***9.99
   ✔> Given ASCII codepage
   ✔  Given a copybook with edited PIC:
   ✔  And binary data: "**12.34"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Decode check-protect field with zero value
   ✔> Given ASCII codepage
   ✔  Given a copybook with edited PIC:
   ✔  And binary data: "***0.00"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Decode leading-plus sign-edited field positive value
   ✔> Given ASCII codepage
   ✔  Given a copybook with edited PIC:
   ✔  And binary data: "+ 123"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Decode leading-minus sign-edited field negative value
   ✔> Given ASCII codepage
   ✔  Given a copybook with edited PIC:
   ✔  And binary data: "-  42"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Decode leading-minus sign-edited field positive value
   ✔> Given ASCII codepage
   ✔  Given a copybook with edited PIC:
   ✔  And binary data: "  123"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Decode BLANK WHEN ZERO field with zero value
   ✔> Given ASCII codepage
   ✔  Given a copybook with edited PIC:
   ✔  And binary data: "    "
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Decode BLANK WHEN ZERO field with non-zero value
   ✔> Given ASCII codepage
   ✔  Given a copybook with edited PIC:
   ✔  And binary data: "  42"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Round-trip edited PIC with zero suppression
   ✔> Given ASCII codepage
   ✔  Given a copybook with edited PIC:
   ✔  And binary data: " 123"
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
   ✔  And decoding should succeed
   ✔  And encoding should succeed
  Scenario: Round-trip edited PIC with currency and comma
   ✔> Given ASCII codepage
   ✔  Given a copybook with edited PIC:
   ✔  And binary data: "$123"
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
   ✔  And decoding should succeed
   ✔  And encoding should succeed
  Scenario: Round-trip edited PIC with check-protect
   ✔> Given ASCII codepage
   ✔  Given a copybook with edited PIC:
   ✔  And binary data: "*123"
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
   ✔  And decoding should succeed
   ✔  And encoding should succeed
  Scenario: Round-trip edited PIC with leading sign
   ✔> Given ASCII codepage
   ✔  Given a copybook with edited PIC:
   ✔  And binary data: "+123"
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
   ✔  And decoding should succeed
   ✔  And encoding should succeed
Feature: Edited PIC E3 Encoding
  Scenario: Encode with basic Z-editing (zero suppression)
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 4 bytes
  Scenario: Encode with Z-editing and leading zeros suppressed
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 4 bytes
  Scenario: Encode with Z-editing and zero value
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 4 bytes
  Scenario: Encode with leading plus sign (E3.1)
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 4 bytes
  Scenario: Encode with leading plus sign and negative value
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 4 bytes
  Scenario: Encode with leading minus sign (E3.1)
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 4 bytes
  Scenario: Encode with leading minus sign and negative value
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 4 bytes
  Scenario: Encode with trailing plus sign (E3.2)
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 4 bytes
  Scenario: Encode with trailing plus sign and negative value
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 4 bytes
  Scenario: Encode with trailing minus sign (E3.2)
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 4 bytes
  Scenario: Encode with trailing minus sign and negative value
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 4 bytes
  Scenario: Encode with CR credit indicator (E3.3)
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 5 bytes
  Scenario: Encode with CR credit indicator and negative value
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 5 bytes
  Scenario: Encode with DB debit indicator (E3.3)
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 5 bytes
  Scenario: Encode with DB debit indicator and negative value
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 5 bytes
  Scenario: Encode with comma separator (E3.4)
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 7 bytes
  Scenario: Encode with comma separator and zero suppression
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 7 bytes
  Scenario: Encode with comma separator and decimal
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✘  And the encoded output should be 10 bytes
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\edited_pic_encoding.feature:291:5
      Matched: tests\bdd\steps\encode_decode.rs:590:1
      Step panicked. Captured output: assertion `left == right` failed: Expected encoded output to be 10 bytes, got 9
        left: 9
       right: 10
  Scenario: Encode with asterisk check protection (E3.5)
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 4 bytes
  Scenario: Encode with asterisk check protection and small value
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 4 bytes
  Scenario: Encode with asterisk check protection and zero value
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 4 bytes
  Scenario: Encode with asterisk check protection and decimal
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 6 bytes
  Scenario: Encode with currency symbol (E3.6)
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 4 bytes
  Scenario: Encode with currency symbol and zero suppression
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 5 bytes
  Scenario: Encode with currency symbol and decimal
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 7 bytes
  Scenario: Encode with currency symbol and comma separator
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✘  And the encoded output should be 10 bytes
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\edited_pic_encoding.feature:419:5
      Matched: tests\bdd\steps\encode_decode.rs:590:1
      Step panicked. Captured output: assertion `left == right` failed: Expected encoded output to be 10 bytes, got 9
        left: 9
       right: 10
  Scenario: Encode with space insertion (E3.7)
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✘  And the encoded output should be 7 bytes
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\edited_pic_encoding.feature:438:5
      Matched: tests\bdd\steps\encode_decode.rs:590:1
      Step panicked. Captured output: assertion `left == right` failed: Expected encoded output to be 7 bytes, got 0
        left: 0
       right: 7
  Scenario: Encode with multiple space insertions
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✘  And the encoded output should be 5 bytes
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\edited_pic_encoding.feature:453:5
      Matched: tests\bdd\steps\encode_decode.rs:590:1
      Step panicked. Captured output: assertion `left == right` failed: Expected encoded output to be 5 bytes, got 0
        left: 0
       right: 5
  Scenario: Encode with space insertion and zero suppression
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✘  And the encoded output should be 7 bytes
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\edited_pic_encoding.feature:468:5
      Matched: tests\bdd\steps\encode_decode.rs:590:1
      Step panicked. Captured output: assertion `left == right` failed: Expected encoded output to be 7 bytes, got 6
        left: 6
       right: 7
  Scenario: Encode with space insertion and decimal
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✘  Then encoding should succeed
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\edited_pic_encoding.feature:482:5
      Matched: tests\bdd\steps\encode_decode.rs:568:1
      Step panicked. Captured output: Encoding failed with error: CBKP001_SYNTAX: Invalid level number '99' (line 3)
  Scenario: Encode with decimal and zero suppression
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 6 bytes
  Scenario: Encode with decimal and zero insert
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✘  And the encoded output should be 5 bytes
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\edited_pic_encoding.feature:517:5
      Matched: tests\bdd\steps\encode_decode.rs:590:1
      Step panicked. Captured output: assertion `left == right` failed: Expected encoded output to be 5 bytes, got 0
        left: 0
       right: 5
  Scenario: Encode with zero insert pattern
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 4 bytes
  Scenario: Encode with zero insert and decimal
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 7 bytes
  Scenario: Encode with currency, asterisk, and decimal
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 8 bytes
  Scenario: Encode with leading plus and zero suppression
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 4 bytes
  Scenario: Encode with trailing plus and zero suppression
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 5 bytes
  Scenario: Encode with value overflow should fail
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✘  Then encoding should fail
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\edited_pic_encoding.feature:610:5
      Matched: tests\bdd\steps\encode_decode.rs:624:1
      Step panicked. Captured output: Encoding should fail
  Scenario: Encode with invalid character in value should fail
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✘  Then encoding should fail
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\edited_pic_encoding.feature:624:5
      Matched: tests\bdd\steps\encode_decode.rs:624:1
      Step panicked. Captured output: Encoding should fail
  Scenario: Encode with empty value should fail
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✘  Then encoding should fail
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\edited_pic_encoding.feature:638:5
      Matched: tests\bdd\steps\encode_decode.rs:624:1
      Step panicked. Captured output: Encoding should fail
  Scenario: Encode with multiple decimal points in value should fail
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✘  Then encoding should fail
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\edited_pic_encoding.feature:652:5
      Matched: tests\bdd\steps\encode_decode.rs:624:1
      Step panicked. Captured output: Encoding should fail
  Scenario: Round-trip edited PIC with Z-editing
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And binary data: " 123"
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
   ✔  And decoding should succeed
   ✔  And encoding should succeed
  Scenario: Round-trip edited PIC with leading plus
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And binary data: "+123"
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
   ✔  And decoding should succeed
   ✔  And encoding should succeed
  Scenario: Round-trip edited PIC with CR indicator
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And binary data: "123  "
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
   ✔  And decoding should succeed
   ✔  And encoding should succeed
  Scenario: Round-trip edited PIC with comma separator
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And binary data: "123,456"
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
   ✔  And decoding should succeed
   ✔  And encoding should succeed
  Scenario: Round-trip edited PIC with currency symbol
   ✔  Given a copybook with edited PIC:
   ✔  And ASCII codepage
   ✔  And binary data: "$123"
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
   ✔  And decoding should succeed
   ✔  And encoding should succeed
Feature: Encode and Decode Operations
  Scenario: Decode ASCII binary data to JSON
   ✔  Given a simple copybook with a single field
   ✔  And ASCII codepage
   ✔  And binary data: "ABCDEFGHIJ"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
   ✔  And the decoded output should contain "TEST-FIELD"
  Scenario: Encode JSON data to ASCII binary
   ✔  Given a simple copybook with a single field
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 10 bytes
  Scenario: Round-trip ASCII data losslessly
   ✔  Given a simple copybook with a single field
   ✔  And ASCII codepage
   ✔  And binary data: "ABCDEFGHIJ"
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
   ✔  And decoding should succeed
   ✔  And encoding should succeed
  Scenario: Decode numeric fields from binary
   ✔  Given a copybook with numeric fields
   ✔  And ASCII codepage
   ✔  And binary data: "000000000000000000"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
   ✘  And the decoded output should contain "PACKED-DECIMAL"
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\encode_decode.feature:43:5
      Matched: tests\bdd\steps\encode_decode.rs:424:1
      Step panicked. Captured output: Expected decoded output to contain 'PACKED-DECIMAL', got: 
  Scenario: Encode numeric fields to binary
   ✔  Given a copybook with numeric fields
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
  Scenario: Decode array fields from binary
   ✔  Given a copybook with OCCURS clause
   ✔  And ASCII codepage
   ✔  And binary data: "ELEMENT001ELEMENT002ELEMENT003ELEMENT004ELEMENT005"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
   ✔  And the decoded output should contain "ELEMENT"
  Scenario: Encode array fields to binary
   ✔  Given a copybook with OCCURS clause
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 50 bytes
  Scenario: Decode with EBCDIC codepage
   ✔  Given a simple copybook with a single field
   ✔  And EBCDIC codepage
   ✔  And binary data: "\xC1\xC2\xC3\xC4\xC5\xC6\xC7\xC8\xC9\xD1"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Encode with EBCDIC codepage
   ✔  Given a simple copybook with a single field
   ✔  And EBCDIC codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 10 bytes
  Scenario: Round-trip with EBCDIC codepage
   ✔  Given a simple copybook with a single field
   ✔  And EBCDIC codepage
   ✔  And binary data: "\xC1\xC2\xC3\xC4\xC5\xC6\xC7\xC8\xC9\xD1"
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
   ✔  And decoding should succeed
   ✔  And encoding should succeed
  Scenario: Decode with metadata enabled
   ✔  Given a simple copybook with a single field
   ✔  And ASCII codepage
   ✔  And binary data: "ABCDEFGHIJ"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
   ✔  And the decoded output should contain "record_index"
   ✔  And the decoded output should contain "codepage"
  Scenario: Encode with coercion enabled
   ✔  Given a copybook with numeric fields
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
  Scenario: Decode nested group structures
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "ABCDE12345FGHIJKLMNO"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
   ✔  And the decoded output should contain "SUB-FIELD-1"
   ✔  And the decoded output should contain "SUB-FIELD-2"
   ✔  And the decoded output should contain "SUB-FIELD-3"
  Scenario: Encode nested group structures
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 20 bytes
  Scenario: Round-trip nested group structures
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "ABCDE12345FGHIJKLMNO"
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
   ✔  And decoding should succeed
   ✔  And encoding should succeed
Feature: Encode Round-Trip and Edge Cases
  Scenario: Round-trip simple alphanumeric field
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "JOHN DOE  "
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
   ✔  And decoding should succeed
   ✔  And encoding should succeed
  Scenario: Round-trip zoned decimal field
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "12345"
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
   ✔  And decoding should succeed
   ✔  And encoding should succeed
  Scenario: Round-trip nested group structure
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "ABCDE12300000000"
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
   ✔  And decoding should succeed
   ✔  And encoding should succeed
  Scenario: Round-trip OCCURS array
   ✔  Given a copybook with OCCURS clause
   ✔  And ASCII codepage
   ✔  And binary data: "ELEMENT001ELEMENT002ELEMENT003ELEMENT004ELEMENT005"
   ✔  When the data is round-tripped
   ✘  Then the round-trip should be lossless
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\encode_roundtrip.feature:56:5
      Matched: tests\bdd\steps\encode_decode.rs:511:1
      Step panicked. Captured output: assertion `left == right` failed: Round-trip should be lossless: original data differs from encoded data
        left: [69, 76, 69, 77, 69, 78, 84, 48, 48, 49, 69, 76, 69, 77, 69, 78, 84, 48, 48, 50, 69, 76, 69, 77, 69, 78, 84, 48, 48, 51, 69, 76, 69, 77, 69, 78, 84, 48, 48, 52, 69, 76, 69, 77, 69, 78, 84, 48, 48, 53]
       right: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
  Scenario: Encode numeric value that overflows field capacity
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\encode_roundtrip.feature:74:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: Encode string that exceeds field length
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\encode_roundtrip.feature:88:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: Decode with field projection selects subset of fields
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And field selection: "FIELD-A,FIELD-C"
   ✔  When field projection is applied
   ✔  Then the projection should succeed
   ✔  And the field "FIELD-A" should be included in projection
   ✔  And the field "FIELD-C" should be included in projection
   ✔  And the field "FIELD-B" should not be included in projection
  Scenario: Projection with nonexistent field produces error
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And field selection: "NONEXISTENT-FIELD"
   ✔  When field projection is applied
   ✔  Then the projection should fail
   ✔  And an error should occur
  Scenario: Round-trip with EBCDIC codepage
   ✔  Given a simple copybook with a single field
   ✔  And EBCDIC codepage
   ✔  And binary data: "\xC1\xC2\xC3\xC4\xC5\xC6\xC7\xC8\xC9\xD1"
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
   ✔  And decoding should succeed
   ✔  And encoding should succeed
  Scenario: Encode zero-value numeric field
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 5 bytes
Feature: Enterprise Audit System
  Scenario: SOX compliance audit trail
   ✔> Given the audit system is enabled
   ✔> And an audit context is initialized
   ✔  Given a financial SOX compliance copybook
   ✔  When the copybook is processed with audit
   ✔  Then the audit event type should be "CopybookParse"
   ✔  And the audit event severity should be "Info"
   ✔  And the audit context should have compliance profile "SOX"
   ✔  And the audit event user should be "bdd_test_user"
   ✔  And the audit event timestamp should be non-empty
   ✔  And the audit event integrity hash should be non-empty
   ✔  And the audit output should be in valid JSON format
  Scenario: HIPAA data processing
   ✔> Given the audit system is enabled
   ✔> And an audit context is initialized
   ✔  Given a healthcare HIPAA compliance copybook
   ✔  When the copybook is processed with audit
   ✔  Then the audit event type should be "CopybookParse"
   ✔  And the audit context should have compliance profile "HIPAA"
   ✔  And the audit context should have security classification "PHI"
   ✔  And the audit context metadata should contain "data_classification"
  Scenario: GDPR audit report
   ✔> Given the audit system is enabled
   ✔> And an audit context is initialized
   ✔  Given a GDPR personal data copybook
   ✔  When the copybook is processed with audit
   ✔  Then the audit event type should be "CopybookParse"
   ✔  And the audit context should have compliance profile "GDPR"
   ✔  And the audit context metadata should contain "legal_basis"
   ✔  And the audit context metadata should contain "processing_purpose"
  Scenario: PCI DSS compliance
   ✔> Given the audit system is enabled
   ✔> And an audit context is initialized
   ✔  Given a PCI DSS payment card copybook
   ✔  When the copybook is processed with audit
   ✔  Then the audit event type should be "CopybookParse"
   ✔  And the audit context should have compliance profile "PciDss"
   ✔  And the audit context metadata should contain "cardholder_data"
  Scenario: Decode operation audit
   ✔> Given the audit system is enabled
   ✔> And an audit context is initialized
   ✔  Given a simple audit copybook
   ✔  When a decode audit event is created
   ✔  Then the audit event type should be "DataTransformation"
   ✔  And the audit payload should contain "Decode"
   ✔  And the audit payload should contain "records_processed"
  Scenario: Encode operation audit
   ✔> Given the audit system is enabled
   ✔> And an audit context is initialized
   ✔  Given a simple audit copybook
   ✔  When an encode audit event is created
   ✔  Then the audit event type should be "DataTransformation"
   ✔  And the audit payload should contain "Encode"
  Scenario: Field projection audit
   ✔> Given the audit system is enabled
   ✔> And an audit context is initialized
   ✔  Given a simple audit copybook
   ✔  When a projection audit event is created
   ✔  Then the audit event type should be "DataValidation"
   ✔  And the audit payload should contain "validation_rules"
  Scenario: Dialect configuration change audit
   ✔> Given the audit system is enabled
   ✔> And an audit context is initialized
   ✔  Given a simple audit copybook
   ✔  When a configuration change audit event is created
   ✔  Then the audit event type should be "ConfigurationChange"
   ✔  And the audit payload should contain "old_configuration"
   ✔  And the audit payload should contain "change_reason"
  Scenario: Security classification
   ✔> Given the audit system is enabled
   ✔> And an audit context is initialized
   ✔  Given security classification "Confidential"
   ✔  When a security event is created
   ✔  Then the audit event type should be "SecurityEvent"
   ✔  And the audit event severity should be "High"
   ✔  And the audit context should have security classification "Confidential"
  Scenario: Access event success
   ✔> Given the audit system is enabled
   ✔> And an audit context is initialized
   ✔  When an access event is created with result "Success"
   ✔  Then the audit event type should be "AccessEvent"
   ✔  And the audit event severity should be "Info"
   ✔  And the audit payload should contain "Success"
  Scenario: Access event denied
   ✔> Given the audit system is enabled
   ✔> And an audit context is initialized
   ✔  When an access event is created with result "Denied"
   ✔  Then the audit event type should be "AccessEvent"
   ✔  And the audit event severity should be "Medium"
   ✔  And the audit payload should contain "Denied"
  Scenario: Error audit event
   ✔> Given the audit system is enabled
   ✔> And an audit context is initialized
   ✔  When an error audit event is created
   ✔  Then the audit event type should be "ErrorEvent"
   ✔  And the audit payload should contain "error_code"
   ✔  And the audit payload should contain "error_message"
   ✔  And the audit payload should contain "user_impact"
  Scenario: Performance measurement
   ✔> Given the audit system is enabled
   ✔> And an audit context is initialized
   ✔  When a performance measurement event is created
   ✔  Then the audit event type should be "PerformanceMeasurement"
   ✔  And the audit payload should contain "metrics"
   ✔  And the audit event severity should be "Info"
  Scenario: Performance regression detected
   ✔> Given the audit system is enabled
   ✔> And an audit context is initialized
   ✔  Given throughput metrics below baseline
   ✔  When a performance measurement event is created
   ✔  Then the audit event type should be "PerformanceMeasurement"
   ✔  And the audit event severity should be "Medium"
   ✔  And the audit payload should contain "regression_detected"
  Scenario: Audit events JSON serialization
   ✔> Given the audit system is enabled
   ✔> And an audit context is initialized
   ✔  Given a financial SOX compliance copybook
   ✔  When the copybook is processed with audit
   ✔  And a security event is created
   ✔  Then the audit trail should have 2 events
   ✔  And all audit events should have required fields
   ✔  And the audit output should be in valid JSON format
  Scenario: Audit chain integrity
   ✔> Given the audit system is enabled
   ✔> And an audit context is initialized
   ✔  Given a financial SOX compliance copybook
   ✔  When the copybook is processed with audit
   ✔  And a security event is created
   ✔  Then the audit chain should be valid
  Scenario: Multi-framework compliance
   ✔> Given the audit system is enabled
   ✔> And an audit context is initialized
   ✔  Given compliance profile "SOX"
   ✔  And compliance profile "HIPAA"
   ✔  And compliance profile "GDPR"
   ✔  When a compliance check event is created
   ✔  Then the audit event type should be "ComplianceCheck"
   ✔  And the audit context should have compliance profile "SOX"
   ✔  And the audit context should have compliance profile "HIPAA"
   ✔  And the audit context should have compliance profile "GDPR"
  Scenario: Child context for nested operations
   ✔> Given the audit system is enabled
   ✔> And an audit context is initialized
   ✔  When a child context is created for "nested_decode"
   ✔  Then the child context should have parent operation
   ✔  And the child context operation id should differ from parent
Feature: Enterprise COBOL Features
  Scenario: Parse copybook with COMP-3 packed decimal
   ✔  Given a copybook with COMP-3 field:
   ✔  When copybook is parsed
   ✔  Then schema should be successfully parsed
   ✔  And field "AMOUNT" should have type "packed_decimal"
  Scenario: Parse copybook with COMP binary integer
   ✔  Given a copybook with COMP field:
   ✔  When copybook is parsed
   ✔  Then schema should be successfully parsed
   ✔  And field "COUNT" should have type "binary_int"
  Scenario: Parse copybook with COMP-4 binary integer
   ✔  Given a copybook with COMP-4 field:
   ✔  When copybook is parsed
   ✘  Then schema should be successfully parsed
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\enterprise_features.feature:34:5
      Matched: tests\bdd\steps\parsing.rs:493:1
      Step panicked. Captured output: Schema parsing failed with error: CBKP001_SYNTAX: Expected field name after level 5
  Scenario: Parse copybook with COMP-5 binary integer
   ✔  Given a copybook with COMP-5 field:
   ✔  When copybook is parsed
   ✘  Then schema should be successfully parsed
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\enterprise_features.feature:44:5
      Matched: tests\bdd\steps\parsing.rs:493:1
      Step panicked. Captured output: Schema parsing failed with error: CBKP001_SYNTAX: Expected field name after level 5
  Scenario: Parse copybook with SIGN SEPARATE LEADING
   ✔  Given a copybook with SIGN SEPARATE LEADING:
   ✔  When copybook is parsed
   ✔  Then schema should be successfully parsed
   ✔  And field "AMOUNT" should have sign separate placement "leading"
  Scenario: Parse copybook with SIGN SEPARATE TRAILING
   ✔  Given a copybook with SIGN SEPARATE TRAILING:
   ✔  When copybook is parsed
   ✔  Then schema should be successfully parsed
   ✔  And field "AMOUNT" should have sign separate placement "trailing"
  Scenario: Parse copybook with BLANK WHEN ZERO
   ✔  Given a copybook with BLANK WHEN ZERO:
   ✔  When copybook is parsed
   ✔  Then schema should be successfully parsed
   ✔  And field "AMOUNT" should have blank_when_zero true
  Scenario: Parse copybook with SYNCHRONIZED
   ✔  Given a copybook with SYNCHRONIZED:
   ✔  When copybook is parsed
   ✔  Then schema should be successfully parsed
   ✔  And field "FIELD1" should have synchronized true
  Scenario: Parse copybook with RENAMES
   ✔  Given a copybook with RENAMES:
   ✔  When copybook is parsed
   ✔  Then schema should be successfully parsed
   ✔  And field "ALIAS" should have level 66
  Scenario: Parse copybook with multiple RENAMES
   ✔  Given a copybook with multiple RENAMES:
   ✔  When copybook is parsed
   ✔  Then schema should be successfully parsed
   ✔  And there should be 2 level-66 fields
  Scenario: Parse copybook with OCCURS DEPENDING ON (ODO)
   ✔  Given a copybook with ODO:
   ✔  When copybook is parsed
   ✔  Then schema should be successfully parsed
   ✘  And field "ARRAY" should have ODO with counter "COUNT"
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\enterprise_features.feature:125:5
      Step match is ambiguous: Possible matches:
      ^field (?:"(?P<__0_0>[^"\\]*(?:\\.[^"\\]*)*)"|'(?P<__0_1>[^'\\]*(?:\\.[^'\\]*)*)') should have ODO with counter (?:"(?P<__1_0>[^"\\]*(?:\\.[^"\\]*)*)"|'(?P<__1_1>[^'\\]*(?:\\.[^'\\]*)*)')$ --> tests\bdd\steps\parsing.rs:316:1
      ^field ([^\s]+) should have ODO with counter (?:"(?P<__1_0>[^"\\]*(?:\\.[^"\\]*)*)"|'(?P<__1_1>[^'\\]*(?:\\.[^'\\]*)*)')$ --> tests\bdd\steps\parsing.rs:292:1
  Scenario: Parse copybook with nested ODO
   ✔  Given a copybook with nested ODO:
   ✔  When copybook is parsed
   ✘  Then schema should be successfully parsed
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\enterprise_features.feature:138:5
      Matched: tests\bdd\steps\parsing.rs:493:1
      Step panicked. Captured output: Schema parsing failed with error: CBKP022_NESTED_ODO: Nested ODO not supported: field 'NESTED-ODO.OUTER-ARRAY.INNER-ARRAY' has OCCURS DEPENDING ON inside another OCCURS/ODO array
  Scenario: Parse copybook with Level-88 condition
   ✔  Given a copybook with Level-88:
   ✔  When copybook is parsed
   ✔  Then schema should be successfully parsed
   ✔  And field "ACTIVE" should have level 88
   ✔  And field "INACTIVE" should have level 88
   ✔  And field "PENDING" should have level 88
  Scenario: Parse copybook with Level-88 VALUE THROUGH
   ✔  Given a copybook with Level-88 VALUE THROUGH:
   ✔  When copybook is parsed
   ✔  Then schema should be successfully parsed
  Scenario: Parse copybook with edited numeric PIC
   ✔  Given a copybook with edited PIC:
   ✔  When copybook is parsed
   ✔  Then schema should be successfully parsed
   ✔  And field "AMOUNT" should have type "edited_numeric"
  Scenario: Parse copybook with multiple edited PIC formats
   ✔  Given a copybook with multiple edited PICs:
   ✔  When copybook is parsed
   ✔  Then schema should be successfully parsed
  Scenario: Decode COMP-3 packed decimal data
   ✔  Given a copybook with COMP-3 field:
   ✔  And ASCII codepage
   ✔  And binary data for value 12345.67
   ✔  When binary data is decoded
   ✔  Then decoded value should be "12345.67"
  Scenario: Reject corrupted COMP-3 packed decimal data
   ✔  Given a copybook with COMP-3 field:
   ✔  And strict mode
   ✔  And binary data: "\xA2\x34\x5C"
   ✔  When the binary data is decoded
   ✔  Then decoding should fail
   ✔  And error code should be "CBKD401_COMP3_INVALID_NIBBLE"
  Scenario: Encode COMP-3 packed decimal data
   ✔  Given a copybook with COMP-3 field:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When JSON data is encoded
   ✔  Then encoding should succeed
   ✘  And encoded length should be 4 bytes
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\enterprise_features.feature:224:5
      Matched: tests\bdd\steps\encode_decode.rs:641:1
      Step panicked. Captured output: assertion `left == right` failed: Expected encoded length 4 bytes, got 5
        left: 5
       right: 4
  Scenario: Decode binary integer data
   ✔  Given a copybook with COMP field:
   ✔  And ASCII codepage
   ✔  And binary data for value 123456789
   ✔  When binary data is decoded
   ✔  Then decoded value should be "123456789"
  Scenario: Encode binary integer data
   ✔  Given a copybook with COMP field:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And encoded length should be 4 bytes
  Scenario: Decode SIGN SEPARATE data
   ✔  Given a copybook with SIGN SEPARATE LEADING:
   ✔  And ASCII codepage
   ✔  And binary data for value -12345
   ✔  When binary data is decoded
   ✔  Then decoded value should be "-12345"
  Scenario: Encode SIGN SEPARATE data
   ✔  Given a copybook with SIGN SEPARATE LEADING:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When JSON data is encoded
   ✔  Then encoding should succeed
  Scenario: Decode ODO array data
   ✔  Given a copybook with ODO:
   ✔  And ASCII codepage
   ✔  And binary data with COUNT=3 and 3 elements
   ✔  When binary data is decoded
   ✔  Then decoded field COUNT should be "003"
   ✔  And there should be 3 ARRAY elements
  Scenario: Encode ODO array data
   ✔  Given a copybook with ODO:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When JSON data is encoded
   ✔  Then encoding should succeed
  Scenario: Parse complex nested structure
   ✔  Given a copybook with nested groups:
   ✔  When copybook is parsed
   ✔  Then schema should be successfully parsed
   ✔  And there should be 5 leaf fields
  Scenario: Decode complex nested structure
   ✔  Given a copybook with nested groups:
   ✔  And ASCII codepage
   ✔  And binary data for all fields
   ✔  When binary data is decoded
   ✔  Then all fields should be decoded
   ✘  And FIELD1 should be "ABCDEFGHIJ"
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\enterprise_features.feature:341:5
      Matched: tests\bdd\steps\encode_decode.rs:676:1
      Step panicked. Captured output: assertion `left == right` failed: Expected FIELD1='ABCDEFGHIJ', got ''
        left: ""
       right: "ABCDEFGHIJ"
  Scenario: Encode complex nested structure
   ✔  Given a copybook with nested groups:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When JSON data is encoded
   ✔  Then encoding should succeed
  Scenario: Handle BLANK WHEN ZERO on decode
   ✔  Given a copybook with BLANK WHEN ZERO:
   ✔  And ASCII codepage
   ✔  And binary data with zero value
   ✔  When binary data is decoded
   ✘  Then decoded field AMOUNT should be blank
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\enterprise_features.feature:377:5
      Matched: tests\bdd\steps\encode_decode.rs:482:1
      Step panicked. Captured output: Expected decoded field 'AMOUNT' to be blank, got '0'
  Scenario: Handle BLANK WHEN ZERO on encode
   ✔  Given a copybook with BLANK WHEN ZERO:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When JSON data is encoded
   ✔  Then encoding should succeed
   ✘  And encoded data should be blank
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\enterprise_features.feature:392:5
      Matched: tests\bdd\steps\encode_decode.rs:712:1
      Step panicked. Captured output: Expected encoded data to be blank
  Scenario: Parse copybook with multiple record types
   ✔  Given a copybook with diverse field types:
   ✔  When copybook is parsed
   ✔  Then schema should be successfully parsed
   ✔  And there should be 5 fields
   ✔  And field types should be diverse
  Scenario: Decode EBCDIC data with ASCII output
   ✔  Given a copybook with text field:
   ✔  And EBCDIC codepage
   ✔  And EBCDIC binary data
   ✔  When binary data is decoded
   ✔  Then decoded value should be converted to ASCII
  Scenario: Encode ASCII data with EBCDIC output
   ✔  Given a copybook with text field:
   ✔  And EBCDIC codepage
   ✔  And ASCII JSON data
   ✔  When JSON data is encoded
   ✔  Then encoded data should be in EBCDIC
  Scenario: Parse copybook with large OCCURS
   ✔  Given a copybook with large OCCURS:
   ✔  When copybook is parsed
   ✔  Then schema should be successfully parsed
   ✘  And field "ARRAY" should have OCCURS count 100
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\enterprise_features.feature:440:5
      Step match is ambiguous: Possible matches:
      ^field (?:"(?P<__0_0>[^"\\]*(?:\\.[^"\\]*)*)"|'(?P<__0_1>[^'\\]*(?:\\.[^'\\]*)*)') should have OCCURS count ((?:-?\d+)|(?:\d+))$ --> tests\bdd\steps\parsing.rs:283:1
      ^field ([^\s]+) should have OCCURS count ((?:-?\d+)|(?:\d+))$ --> tests\bdd\steps\parsing.rs:260:1
  Scenario: Decode large OCCURS array
   ✔  Given a copybook with large OCCURS:
   ✔  And ASCII codepage
   ✔  And binary data with 100 elements
   ✔  When binary data is decoded
   ✔  Then there should be 100 ARRAY elements
  Scenario: Encode large OCCURS array
   ✔  Given a copybook with large OCCURS:
   ✔  And ASCII codepage
   ✔  And JSON data with 100 elements
   ✔  When JSON data is encoded
   ✔  Then encoding should succeed
  Scenario: Parse copybook with mixed level numbers
   ✔  Given a copybook with various levels:
   ✔  When copybook is parsed
   ✔  Then schema should be successfully parsed
   ✔  And fields should have correct levels
  Scenario: Parse copybook with FILLER fields
   ✔  Given a copybook with FILLER:
   ✔  When copybook is parsed
   ✔  Then schema should be successfully parsed
   ✔  And field "FILLER" should be present
  Scenario: Decode with FILLER fields excluded
   ✔  Given a copybook with FILLER:
   ✔  And ASCII codepage
   ✔  And binary data
   ✔  And emit_filler is false
   ✔  When binary data is decoded
   ✔  Then FILLER should not be in output
   ✔  And FIELD1 should be present
   ✔  And FIELD2 should be present
  Scenario: Decode with FILLER fields included
   ✔  Given a copybook with FILLER:
   ✔  And ASCII codepage
   ✔  And binary data
   ✔  And emit_filler is true
   ✔  When binary data is decoded
   ✔  Then FILLER should be in output
   ✔  And FIELD1 should be present
   ✔  And FIELD2 should be present
Feature: Error Handling and Edge Cases
  Scenario: Parse copybook with syntax error
   ✔  Given an invalid copybook with syntax error
   ✔  When the copybook is parsed
   ✔  Then an error should occur
   ✔  And the error message should contain "syntax"
  Scenario: Parse copybook with invalid OCCURS clause
   ✔  Given a copybook with invalid OCCURS clause
   ✔  When the copybook is parsed
   ✔  Then an error should occur
   ✔  And the error message should contain "OCCURS"
  Scenario: Parse copybook with invalid PIC clause
   ✔  Given a copybook with invalid PIC clause
   ✔  When the copybook is parsed
   ✔  Then an error should occur
   ✔  And the error message should contain "PIC"
  Scenario: Decode binary data that is too short
   ✔  Given a simple copybook with a single field
   ✔  And ASCII codepage
   ✔  And binary data that is too short
   ✔  When the binary data is decoded
   ✔  Then an error should occur
  Scenario: Decode binary data with invalid encoding
   ✔  Given a simple copybook with a single field
   ✔  And EBCDIC codepage
   ✔  And binary data with invalid encoding
   ✔  When the binary data is decoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\error_handling.feature:37:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: Encode JSON data with missing required fields
   ✔  Given a simple copybook with a single field
   ✔  And ASCII codepage
   ✔  And JSON data with missing required fields
   ✔  When the JSON data is encoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\error_handling.feature:44:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: Encode JSON data with invalid field types
   ✔  Given a simple copybook with a single field
   ✔  And ASCII codepage
   ✔  And JSON data with invalid field types
   ✔  When the JSON data is encoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\error_handling.feature:51:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: Parse copybook with duplicate field names
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\error_handling.feature:61:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: Parse copybook with invalid level numbers
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then an error should occur
  Scenario: Parse copybook with missing level numbers
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\error_handling.feature:79:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: Parse copybook with unterminated string
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then an error should occur
  Scenario: Parse copybook with invalid REDEFINES
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then an error should occur
  Scenario: Parse copybook with circular REDEFINES
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then an error should occur
  Scenario: Decode with ODO count exceeding maximum
   ✔  Given a copybook with ODO (OCCURS DEPENDING ON)
   ✔  And ASCII codepage
   ✔  And binary data: "999ABCDEFGH"
   ✔  When the binary data is decoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\error_handling.feature:115:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: Decode with ODO count below minimum
   ✔  Given a copybook with ODO (OCCURS DEPENDING ON)
   ✔  And ASCII codepage
   ✔  And binary data: "000ABCDEFGH"
   ✔  When the binary data is decoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\error_handling.feature:122:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: Encode JSON with invalid ODO count
   ✔  Given a copybook with ODO (OCCURS DEPENDING ON)
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\error_handling.feature:132:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: Parse copybook with Level-88 without parent
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then an error should occur
  Scenario: Parse copybook with Level-88 without VALUE clause
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\error_handling.feature:150:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: Decode empty binary data
   ✔  Given a simple copybook with a single field
   ✔  And ASCII codepage
   ✔  And binary data: ""
   ✔  When the binary data is decoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\error_handling.feature:157:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: Encode empty JSON data
   ✔  Given a simple copybook with a single field
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\error_handling.feature:166:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: Decode malformed JSONL data
   ✔  Given a simple copybook with a single field
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then an error should occur
  Scenario: Parse copybook with edited PIC
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ✔  And the field "EDITED-FIELD" should have type "edited"
  Scenario: Parse copybook with nested ODO
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then an error should occur
Feature: Error Handling Strategies
  Scenario: Strict mode rejects invalid data
   ✔> Given a copybook with content:
   ✔  Given ASCII codepage
   ✔  And strict mode
   ✔  And binary data: "AB"
   ✔  When the binary data is decoded
   ✔  Then decoding should fail
  Scenario: Lenient mode handles short data
   ✔> Given a copybook with content:
   ✔  Given ASCII codepage
   ✔  And lenient mode
   ✔  And binary data: "HELLOWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: Max errors limits error accumulation
   ✔> Given a copybook with content:
   ✔  Given ASCII codepage
   ✔  And max errors 5
   ✔  And binary data: "HELLOWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: Max errors unlimited allows all errors
   ✔> Given a copybook with content:
   ✔  Given ASCII codepage
   ✔  And max errors unlimited
   ✔  And binary data: "HELLOWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: Strict mode with valid data succeeds
   ✔> Given a copybook with content:
   ✔  Given ASCII codepage
   ✔  And strict mode
   ✔  And binary data: "HELLOWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: Lenient mode with valid data succeeds
   ✔> Given a copybook with content:
   ✔  Given ASCII codepage
   ✔  And lenient mode
   ✔  And binary data: "HELLOWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: Max errors with encoding
   ✔> Given a copybook with content:
   ✔  Given ASCII codepage
   ✔  And max errors 3
   ✔  And JSON data: "{\"DATA-FIELD\":\"TESTDATA01\"}"
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
  Scenario: Strict mode encoding with valid JSON
   ✔> Given a copybook with content:
   ✔  Given ASCII codepage
   ✔  And strict mode
   ✔  And JSON data: "{\"DATA-FIELD\":\"TESTDATA01\"}"
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
  Scenario: Lenient mode encoding
   ✔> Given a copybook with content:
   ✔  Given ASCII codepage
   ✔  And lenient mode
   ✔  And JSON data: "{\"DATA-FIELD\":\"TESTDATA01\"}"
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
  Scenario: Max errors with round-trip
   ✔> Given a copybook with content:
   ✔  Given ASCII codepage
   ✔  And max errors 10
   ✔  And binary data: "HELLOWORLD"
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
  Scenario: Strict mode with numeric field
   ✔> Given a copybook with content:
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And strict mode
   ✔  And binary data: "12345"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: Max errors set to 1
   ✔> Given a copybook with content:
   ✔  Given ASCII codepage
   ✔  And max errors 1
   ✔  And binary data: "HELLOWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
Feature: Error Taxonomy Coverage
  Scenario: CBKP parse error on invalid syntax
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then an error should occur
   ✔  And the error code should start with "CBKP"
   ✔  And the error message should contain context
  Scenario: CBKP parse error on invalid PIC clause
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then an error should occur
   ✔  And the error code should start with "CBKP"
  Scenario: CBKP parse error on nested ODO
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then an error should occur
   ✔  And the error code should start with "CBKP"
  Scenario: CBKS schema error on projection with nonexistent field
   ✔  Given a copybook with content:
   ✔  And field selection: "NONEXISTENT"
   ✔  When the copybook is parsed
   ✔  And field projection is applied
   ✔  Then an error should occur
   ✔  And the error code should start with "CBKS"
  Scenario: CBKD data error on truncated record
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "SHORT"
   ✔  When the binary data is decoded
   ✔  Then an error should occur
   ✘  And the error code should start with "CBKD"
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\error_taxonomy.feature:70:5
      Matched: tests\bdd\steps\error_taxonomy.rs:6:1
      Step panicked. Captured output: Expected error code starting with 'CBKD', got 'CBKF221_RDW_UNDERFLOW'
  Scenario: CBKD data error on invalid COMP-3 nibble
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "\xFF\xFF\xFF"
   ✔  When the binary data is decoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\error_taxonomy.feature:81:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: CBKE encode error on numeric overflow
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\error_taxonomy.feature:98:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: CBKE encode error on string length violation
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✘  Then an error should occur
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\error_taxonomy.feature:113:5
      Matched: tests\bdd\steps\error_handling.rs:6:1
      Step panicked. Captured output: An error should have occurred
  Scenario: Parse errors include line context
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then an error should occur
   ✔  And the error message should contain context
  Scenario: Error code display format is stable
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then an error should occur
   ✔  And the error display should contain "CBK"
Feature: Field Projection
  Scenario: Decode with simple field selection
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "000123JOHN DOE                           123 MAIN STREET                        "
   ✔  And field selection: "CUSTOMER-ID"
   ✔  When the copybook is parsed
   ✔  And the schema is projected with selected fields
   ✔  Then the projection should succeed
   ?  And the projected schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\field_projection.feature:24:5
  Scenario: Decode with multiple field selection
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "000123JOHN DOE                           123 MAIN STREET                        555-1234567890  "
   ✔  And field selection: "CUSTOMER-ID,CUSTOMER-NAME"
   ✔  When the copybook is parsed
   ✔  And the schema is projected with selected fields
   ✔  Then the projection should succeed
   ?  And the projected schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\field_projection.feature:43:5
  Scenario: Decode with group selection
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "000123JOHN DOE                           00000001020240117"
   ✔  And field selection: "CUSTOMER-INFO"
   ✔  When the copybook is parsed
   ✔  And the schema is projected with selected fields
   ✔  Then the projection should succeed
   ?  And the projected schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\field_projection.feature:65:5
  Scenario: Decode with ODO auto-counter inclusion
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "00200000000100000100000000000002000000020000020000000000000"
   ✔  And field selection: "ORDERS"
   ✔  When the copybook is parsed
   ✔  And the schema is projected with selected fields
   ✔  Then the projection should succeed
   ?  And the projected schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\field_projection.feature:86:5
  Scenario: Decode with RENAMES alias resolution
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "JOHN              DOE               M123 MAIN STREET                        "
   ✔  And field selection: "FULL-NAME"
   ✔  When the copybook is parsed
   ✔  And the schema is projected with selected fields
   ✔  Then the projection should succeed
   ?  And the projected schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\field_projection.feature:106:5
  Scenario: Decode with RENAMES alias and regular fields
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And binary data: "JOHN              DOE               000123"
   ✔  And field selection: "FULL-NAME,CUSTOMER-ID"
   ✔  When the copybook is parsed
   ✔  And the schema is projected with selected fields
   ✔  Then the projection should succeed
   ?  And the projected schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\field_projection.feature:127:5
  Scenario: Error when field not found in projection
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And field selection: "NONEXISTENT-FIELD"
   ✔  When the copybook is parsed
   ✔  And the schema is projected with selected fields
   ✔  Then the projection should fail
   ✔  And the error message should contain "NONEXISTENT-FIELD"
   ✔  And the error code should be "CBKS703_PROJECTION_FIELD_NOT_FOUND"
  Scenario: Encode with field projection
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And JSON data:
   ✔  And field selection: "CUSTOMER-ID,CUSTOMER-NAME"
   ✔  When the copybook is parsed
   ✔  And the schema is projected with selected fields
   ✔  Then the projection should succeed
   ?  And the projected schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\field_projection.feature:162:5
  Scenario: Verify with field projection
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And field selection: "TRANSACTION-ID,TRANSACTION-AMT"
   ✔  When the copybook is parsed
   ✔  And the schema is projected with selected fields
   ✔  Then the projection should succeed
   ?  And the projected schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\field_projection.feature:180:5
  Scenario: Empty field selection
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And field selection: ""
   ✔  When the copybook is parsed
   ✔  And the schema is projected with selected fields
   ✔  Then the projection should succeed
   ?  And the projected schema should contain 0 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\field_projection.feature:197:5
Feature: FILLER Handling
  Scenario: Default excludes FILLER from output
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And emit_filler is false
   ✔  And binary data: "HELLOFILLEWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And FILLER should not be in output
  Scenario: emit_filler true includes FILLER in output
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And emit_filler is true
   ✔  And binary data: "HELLOFILLEWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And FILLER should be in output
  Scenario: FILLER naming convention uses byte offset
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And emit_filler is true
   ✔  And binary data: "HELLOPADWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Multiple FILLERs have unique names
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And emit_filler is true
   ✔  And binary data: "ABCXXDEFYYGHI"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Nested FILLER in group
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And emit_filler is true
   ✔  And binary data: "HELLOPADWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: No FILLER fields in copybook
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And emit_filler is true
   ✔  And binary data: "HELLOWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And FILLER should not be in output
  Scenario: emit_filler false with round-trip
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And emit_filler is false
   ✔  And binary data: "HELLOWORLD"
   ✔  When the data is round-tripped
   ✘  Then the round-trip should be lossless
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\filler_handling.feature:107:5
      Matched: tests\bdd\steps\encode_decode.rs:511:1
      Step panicked. Captured output: assertion `left == right` failed: Round-trip should be lossless: original data differs from encoded data
        left: [72, 69, 76, 76, 79, 87, 79, 82, 76, 68]
       right: [72, 69, 76, 76, 79, 0, 0, 0, 0, 0]
  Scenario: FILLER only copybook
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And emit_filler is false
   ✔  And binary data: "HELLOWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: FILLER with valid JSON output
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And emit_filler is true
   ✔  And binary data: "HELLOWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: emit_filler word variant with true
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And emit_filler is true
   ✔  And binary data: "HELLOWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And FILLER should be in output
Feature: Fixed Record Framing
  Scenario: Encoded fixed output round-trips through the fixed microcrate
   ✔  Given a copybook with content:
   ✔  And fixed record format
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 8 bytes
   ✔  And the encoded output should round-trip through the fixed microcrate with LRECL 8
  Scenario: Raw binary input is readable through the fixed microcrate
   ✔  Given a simple copybook with a single field
   ✔  And fixed record format
   ✔  And ASCII codepage
   ✔  And binary data: "HELLO     "
   ✔  Then the binary input should be readable by the fixed microcrate with LRECL 10
Feature: Golden Fixtures Validation
  Scenario: Validate golden fixture schema structure
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ?  And the schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\golden_fixtures.feature:19:5
  Scenario: Compare current output against golden fixture
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ✔  And the field "CUSTOMER-ID" should have offset 0
   ✔  And the field "CUSTOMER-ID" should have length 6
   ✔  And the field "CUSTOMER-NAME" should have offset 6
   ✔  And the field "CUSTOMER-NAME" should have length 30
   ✔  And the field "ACCOUNT-BALANCE" should have offset 36
   ✔  And the field "ACCOUNT-BALANCE" should have length 5
   ✔  And the field "LAST-ACTIVITY-DATE" should have offset 41
   ✔  And the field "LAST-ACTIVITY-DATE" should have length 8
   ✔  And the field "STATUS-CODE" should have offset 49
   ✔  And the field "STATUS-CODE" should have length 1
  Scenario: Update golden fixtures when behavior changes
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ?  And the schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\golden_fixtures.feature:58:5
  Scenario: Golden fixture validation for simple copybook type
   ✔  Given a simple copybook with a single field
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ?  And the schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\golden_fixtures.feature:72:5
  Scenario: Golden fixture validation for complex copybook type
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ?  And the schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\golden_fixtures.feature:95:5
  Scenario: Golden fixture validation for ASCII encoding format
   ✔  Given a simple copybook with a single field
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When JSON data is encoded
   ✔  Then encoding should succeed
   ✘  And the encoded output should be 10 bytes
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\golden_fixtures.feature:114:5
      Matched: tests\bdd\steps\encode_decode.rs:590:1
      Step panicked. Captured output: assertion `left == right` failed: Expected encoded output to be 10 bytes, got 0
        left: 0
       right: 10
  Scenario: Golden fixture validation for EBCDIC encoding format
   ✔  Given a simple copybook with a single field
   ✔  And EBCDIC codepage
   ✔  And JSON data:
   ✔  When JSON data is encoded
   ✔  Then encoding should succeed
   ✘  And the encoded output should be 10 bytes
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\golden_fixtures.feature:127:5
      Matched: tests\bdd\steps\encode_decode.rs:590:1
      Step panicked. Captured output: assertion `left == right` failed: Expected encoded output to be 10 bytes, got 0
        left: 0
       right: 10
  Scenario: Golden fixture validation for error scenarios
   ✔  Given a copybook with invalid SIGN SEPARATE
   ✔  When the copybook is parsed
   ✔  Then parsing should fail
   ✔  And parsing should fail with error code "CBKP001"
  Scenario: Golden fixture validation with dialect variations - Normative
   ✔  Given Normative dialect
   ✔  And a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ?  And the schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\golden_fixtures.feature:147:5
  Scenario: Golden fixture validation with dialect variations - Zero-Tolerant
   ✔  Given Zero-Tolerant dialect
   ✔  And a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ?  And the schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\golden_fixtures.feature:161:5
  Scenario: Golden fixture validation with field projection
   ✔  Given a copybook with content:
   ✔  And field selection is ["CUSTOMER-ID", "CUSTOMER-NAME"]
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ?  And the schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\golden_fixtures.feature:178:5
  Scenario: Golden fixture validation with RDW processing
   ✔  Given RDW record format
   ✔  And a copybook with content:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the round-trip should be lossless
   ✔  And decoding should succeed
  Scenario: Golden fixture validation with edited PIC encoding
   ✔  Given a copybook with edited PIC:
   ✔  When the copybook is parsed
   ✔  Then the schema should be successfully parsed
   ?  And the schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\golden_fixtures.feature:210:5
  Scenario: Golden fixture validation for COMP-3 round-trip
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the round-trip should be lossless
   ✔  And decoding should succeed
   ✘  And decoded field RECORD-ID should be "0001"
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\golden_fixtures.feature:236:5
      Matched: tests\bdd\steps\encode_decode.rs:438:1
      Step panicked. Captured output: No records in decoded output
Feature: JSON Number Modes
  Scenario: Lossless mode preserves numeric precision as string
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And lossless number mode
   ✔  And binary data: "001234599"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Lossless mode with large numbers
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And lossless number mode
   ✔  And binary data: "12345678901234599"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Native mode with simple integer
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And native number mode
   ✔  And binary data: "00042"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: Lossless mode with zero value
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And lossless number mode
   ✔  And binary data: "00000"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: Native mode with decimal value
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And native number mode
   ✔  And binary data: "0012345"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: Lossless mode with COMP-3 field
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And lossless number mode
   ✔  And binary data: "\x01\x23\x4C"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: Native mode with COMP-3 field
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And native number mode
   ✔  And binary data: "\x01\x23\x4C"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: Lossless mode round-trip preserves values
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And lossless number mode
   ✔  And binary data: "12345"
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
  Scenario: Lossless mode with all-nines
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And lossless number mode
   ✔  And binary data: "999999999"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: Native mode with binary integer
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And native number mode
   ✔  And binary data: "\x00\x2A"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: Default number mode is lossless
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "12345"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: Lossless mode output is always valid JSON
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And lossless number mode
   ✔  And binary data: "0421234567"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
Feature: Metadata Emission
  Scenario: Emit meta enabled includes metadata fields
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given emit_meta is true
   ✔  And binary data: "HELLOWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should contain "schema_fingerprint"
  Scenario: Emit meta disabled excludes metadata fields
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given emit_meta is false
   ✔  And binary data: "HELLOWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should not contain "schema_fingerprint"
  Scenario: Metadata contains record_index
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given emit_meta is true
   ✔  And binary data: "HELLOWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should contain "record_index"
  Scenario: Metadata with multi-field copybook
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And emit_meta is true
   ✔  And binary data: "HELLO042"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should contain "schema_fingerprint"
  Scenario: Emit meta with COMP-3 field
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And emit_meta is true
   ✔  And binary data: "\x01\x23\x4C"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should contain "schema_fingerprint"
  Scenario: Default emit_meta is enabled
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given emit_meta is true
   ✔  And binary data: "HELLOWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should contain "schema_fingerprint"
  Scenario: Emit meta does not affect round-trip
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given emit_meta is true
   ✔  And binary data: "HELLOWORLD"
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
  Scenario: Emit meta with valid JSON output
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given emit_meta is true
   ✔  And binary data: "TESTDATA10"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
Feature: Options Microcrate Contracts
  Scenario: Decode options builder applies contract fields
   ✔  Given default decode options
   ✔  When decode record format is set to "rdw"
   ✔  And decode json number mode is set to "native"
   ✔  And decode raw mode is set to "record+rdw"
   ✔  Then decode options format should be "rdw"
   ✔  And decode options json number mode should be "native"
   ✔  And decode options raw mode should be "record+rdw"
  Scenario: Encode options builder applies override and float format
   ✔  Given default encode options
   ✔  When encode zoned override is set to "ascii"
   ✔  And encode float format is set to "ibm-hex"
   ✔  Then encode options zoned override should be "ascii"
   ✔  And encode options float format should be "ibm-hex"
  Scenario: Zoned detection recognizes nibble signatures
   ✔  When zoned encoding is detected from byte "0x35"
   ✔  Then detected zoned encoding should be "ascii"
   ✔  When zoned encoding is detected from byte "0xF7"
   ✔  Then detected zoned encoding should be "ebcdic"
Feature: Parallel Processing
  Scenario: Single-threaded decode
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given thread count 1
   ✔  And multi-record binary data with 10 records
   ?  When the data is decoded with 1 thread(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\parallel_processing.feature:16:5
  Scenario: Multi-threaded decode with 2 threads
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given thread count 2
   ✔  And multi-record binary data with 10 records
   ?  When the data is decoded with 2 thread(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\parallel_processing.feature:23:5
  Scenario: Multi-threaded decode with 4 threads
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given thread count 4
   ✔  And multi-record binary data with 20 records
   ?  When the data is decoded with 4 thread(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\parallel_processing.feature:30:5
  Scenario: Multi-threaded decode with 8 threads
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given thread count 8
   ✔  And multi-record binary data with 100 records
   ?  When the data is decoded with 8 thread(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\parallel_processing.feature:37:5
  Scenario: Output determinism across thread counts
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given multi-record binary data with 10 records
   ?  When the data is decoded with 1 thread(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\parallel_processing.feature:43:5
  Scenario: Single record with multiple threads
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given thread count 4
   ✔  And multi-record binary data with 1 records
   ?  When the data is decoded with 4 thread(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\parallel_processing.feature:51:5
  Scenario: Empty data with threads
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given thread count 2
   ✔  And binary data: ""
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: Thread count of 1 is single-threaded
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given thread count 1
   ✔  And binary data for all fields
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Large record count with threads
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given thread count 4
   ✔  And multi-record binary data with 50 records
   ?  When the data is decoded with 4 thread(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\parallel_processing.feature:71:5
  Scenario: Thread count does not affect output validity
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given thread count 2
   ✔  And multi-record binary data with 5 records
   ?  When the data is decoded with 2 thread(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\parallel_processing.feature:78:5
  Scenario: Determinism with 4 threads
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given multi-record binary data with 20 records
   ?  When the data is decoded with 1 thread(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\parallel_processing.feature:84:5
  Scenario: Decode with default thread count
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given binary data for all fields
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
Feature: Parsing Modes
  Scenario: Strict parsing of valid copybook
   ✔  Given strict parsing mode
   ✔  And a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then parsing should succeed
  Scenario: Tolerant parsing of valid copybook
   ✔  Given tolerant parsing mode
   ✔  And a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then parsing should succeed
  Scenario: Default parsing mode succeeds on valid input
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then parsing should succeed
  Scenario: Strict comments mode
   ✔  Given strict_comments mode
   ✔  And a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then parsing should succeed
  Scenario: Inline comments disabled
   ✔  Given inline comments disabled
   ✔  And a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then parsing should succeed
  Scenario: Strict parsing rejects invalid syntax
   ✔  Given strict parsing mode
   ✔  And a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then parsing should fail
  Scenario: Tolerant parsing with OCCURS
   ✔  Given tolerant parsing mode
   ✔  And a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then parsing should succeed
  Scenario: Strict parsing with Level-88
   ✔  Given strict parsing mode
   ✔  And a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then parsing should succeed
  Scenario: Strict parsing with REDEFINES
   ✔  Given strict parsing mode
   ✔  And a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then parsing should succeed
  Scenario: Default mode handles complex copybook
   ✔  Given a copybook with content:
   ✔  When the copybook is parsed
   ✔  Then parsing should succeed
   ?  And the schema should contain 1 top-level field(s)
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\parsing_modes.feature:110:5
Feature: Advanced Field Projection
  Scenario: Select non-existent field returns CBKS703
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And field selection: "DOES-NOT-EXIST"
   ✔  When the copybook is parsed
   ✔  And the schema is projected with selected fields
   ✔  Then the projection should fail
   ✔  And the error code should be "CBKS703_PROJECTION_FIELD_NOT_FOUND"
   ✔  And the error message should contain "DOES-NOT-EXIST"
  Scenario: Select ODO array auto-includes counter field
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And field selection: "ORDER-ITEMS"
   ✔  When the copybook is parsed
   ✔  And the schema is projected with selected fields
   ✔  Then the projection should succeed
   ✔  And the field "ORDER-ITEMS" should be included in projection
   ✔  And the field "ORDER-COUNT" should be included in projection
   ✔  And the field "ITEM-ID" should be included in projection
  Scenario: Select RENAMES alias resolves to storage fields
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And field selection: "FULL-NAME"
   ✔  When the copybook is parsed
   ✔  And the schema is projected with selected fields
   ✔  Then the projection should succeed
   ✔  And the field "FIRST-NAME" should be included in projection
   ✔  And the field "LAST-NAME" should be included in projection
   ✔  And the field "AGE" should not be included in projection
  Scenario: Select multiple fields from nested groups
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And field selection: "ACCT-ID,BALANCE"
   ✔  When the copybook is parsed
   ✔  And the schema is projected with selected fields
   ✔  Then the projection should succeed
   ✔  And the field "ACCT-ID" should be included in projection
   ✔  And the field "BALANCE" should be included in projection
   ✔  And the field "ACCT-NAME" should not be included in projection
   ✔  And the field "CREDIT-LIMIT" should not be included in projection
  Scenario: Select all leaf fields produces identity projection
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And field selection: "FIELD-A,FIELD-B,FIELD-C"
   ✔  When the copybook is parsed
   ✔  And the schema is projected with selected fields
   ✔  Then the projection should succeed
   ✔  And the field "FIELD-A" should be included in projection
   ✔  And the field "FIELD-B" should be included in projection
   ✔  And the field "FIELD-C" should be included in projection
  Scenario: Select group field includes all children
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And field selection: "HEADER-GROUP"
   ✔  When the copybook is parsed
   ✔  And the schema is projected with selected fields
   ✔  Then the projection should succeed
   ✔  And the field "HEADER-GROUP" should be included in projection
   ✔  And the field "HDR-TYPE" should be included in projection
   ✔  And the field "HDR-LEN" should be included in projection
   ✔  And the field "DETAIL-FIELD" should not be included in projection
  Scenario: Select mix of valid and non-existent fields fails
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And field selection: "REAL-FIELD,GHOST-FIELD"
   ✔  When the copybook is parsed
   ✔  And the schema is projected with selected fields
   ✔  Then the projection should fail
   ✔  And the error code should be "CBKS703_PROJECTION_FIELD_NOT_FOUND"
Feature: Raw Data Capture
  Scenario: Raw mode off does not emit __raw_b64
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given raw mode "off"
   ✔  And binary data: "HELLOWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should not contain "__raw_b64"
  Scenario: Raw mode record emits __raw_b64
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given raw mode "record"
   ✔  And binary data: "HELLOWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should contain "__raw_b64"
  Scenario: Raw mode record base64 decodes to original payload
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given raw mode "record"
   ✔  And binary data: "HELLOWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the raw_b64 field should decode to the original binary data
  Scenario: Raw mode field emits field-level raw
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given raw mode "field"
   ✔  And binary data: "HELLOWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: Raw mode record preserves binary literal bytes
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given raw mode "record"
   ✔  And binary data: "\x41\x42\x43\x44\x45\x46\x47\x48\x49\x4A"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the raw_b64 field should decode to the original binary data
  Scenario: Default raw mode is off
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given binary data: "HELLOWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should not contain "__raw_b64"
  Scenario: Raw mode record with COMP-3 field
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And raw mode "record"
   ✔  And binary data: "\x01\x23\x4C"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should contain "__raw_b64"
  Scenario: Raw mode off with multiple fields
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And raw mode "off"
   ✔  And binary data: "HELLOWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should not contain "__raw_b64"
  Scenario: Raw mode record with JSON validity
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given raw mode "record"
   ✔  And binary data: "ABCDEFGHIJ"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
  Scenario: Raw mode with empty field
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given raw mode "record"
   ✔  And binary data: "          "
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should contain "__raw_b64"
  Scenario: use_raw enabled for encode
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given use_raw enabled for encode
   ✔  And raw mode "record"
   ✔  And binary data: "HELLOWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: Raw mode record preserves all data
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given raw mode "record"
   ✔  And binary data: "TESTDATA12"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
   ✔  And the decoded output should contain "__raw_b64"
  Scenario: Raw mode off with round-trip
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given raw mode "off"
   ✔  And binary data: "HELLOWORLD"
   ✔  When the data is round-tripped
   ✔  Then the round-trip should be lossless
  Scenario: Raw mode field with numeric data
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And raw mode "field"
   ✔  And binary data: "12345"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: Multiple records with raw mode record
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And raw mode "record"
   ✔  And binary data: "HELLOWORLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: Raw mode with EBCDIC codepage
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given a copybook with content:
   ✔  And codepage "CP037"
   ✔  And raw mode "record"
   ✔  And binary data for all fields
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should contain "__raw_b64"
  Scenario: Decode output is valid JSON with all raw modes
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given raw mode "off"
   ✔  And binary data: "HELLOWORLD"
   ✔  When the binary data is decoded
   ✔  Then the decoded output should be valid JSON
  Scenario: Raw mode record with round-trip preserves structure
   ✔> Given a copybook with content:
   ✔> And ASCII codepage
   ✔  Given raw mode "record"
   ✔  And binary data: "TESTRECORD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should contain "__raw_b64"
Feature: RDW corruption heuristic
  Scenario: detect an ASCII digit length header
   ✔  When the rdw ascii-corruption heuristic evaluates header "\x31\x32\x00\x00"
   ✔  Then the rdw ascii-corruption heuristic should report ASCII corruption
  Scenario: detect an ascii-corruption RDW header in detector
   ✔  When the rdw corruption detector evaluates header "\x31\x32\x00\x00"
   ✔  Then the rdw corruption detector should report corruption
  Scenario: ignore a non-ASCII length header
   ✔  When the rdw ascii-corruption heuristic evaluates header "\x00\x05\x00\x00"
   ✔  Then the rdw ascii-corruption heuristic should not report ASCII corruption
  Scenario: ignore a clean RDW header in detector
   ✔  When the rdw corruption detector evaluates header "\x00\x05\x00\x00"
   ✔  Then the rdw corruption detector should not report corruption
  Scenario: detect a corrupted EBCDIC control byte
   ✔  When the ebcdic corruption predicate evaluates bytes "\x00"
   ✔  Then the ebcdic corruption predicate should detect corruption
  Scenario: ignore a clean EBCDIC control-free byte sequence
   ✔  When the ebcdic corruption predicate evaluates bytes "\x41\x42\x43"
   ✔  Then the ebcdic corruption predicate should not detect corruption
  Scenario: detect invalid packed-decimal nibbles
   ✔  When the packed corruption predicate evaluates bytes "\xA2\x34\x5A"
   ✔  Then the packed corruption predicate should detect corruption
  Scenario: ignore valid packed-decimal data
   ✔  When the packed corruption predicate evaluates bytes "\x12\x34\x5C"
   ✔  Then the packed corruption predicate should not detect corruption
  Scenario: detect corrupted EBCDIC bytes in detector
   ✔  When the ebcdic corruption detector evaluates bytes "\x00"
   ✔  Then the ebcdic corruption detector should report corruption
  Scenario: ignore clean EBCDIC bytes in detector
   ✔  When the ebcdic corruption detector evaluates bytes "\x41\x42\x43"
   ✔  Then the ebcdic corruption detector should not report corruption
  Scenario: detect packed-decimal corruption in detector
   ✔  When the packed corruption detector evaluates bytes "\xA2\x34\x5A"
   ✔  Then the packed corruption detector should report corruption
  Scenario: ignore valid packed-decimal data in detector
   ✔  When the packed corruption detector evaluates bytes "\x12\x34\x5C"
   ✔  Then the packed corruption detector should not report corruption
Feature: RDW (Record Descriptor Word) Processing
  Scenario: Decode fixed-length record with RDW header
   ✔  Given a simple copybook with a single field
   ✔  And RDW record format
   ✔  And ASCII codepage
   ✔  And binary data: "\x00\x0A\x00\x00HELLO WRLD"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
   ✔  And the decoded output should contain "TEST-FIELD"
   ✔  And decoded field TEST-FIELD should be "HELLO WRLD"
  Scenario: Decode variable-length record with RDW header
   ✔  Given a copybook with content:
   ✔  And RDW record format
   ✔  And ASCII codepage
   ✔  And binary data: "\x00\x08\x00\x00ORIGINAL"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
   ✔  And decoded field VARIABLE-RECORD should be "ORIGINAL"
  Scenario: Encode with RDW header generation
   ✔  Given a copybook with content:
   ✔  And RDW record format
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should be 14 bytes
   ✔  And the encoded output should start with RDW header
   ✔  And the encoded output should round-trip through the RDW microcrate
   ✔  And the encoded output should be readable by the RDW reader microcrate
  Scenario: Round-trip with RDW processing
   ✔  Given a copybook with content:
   ✔  And RDW record format
   ✔  And ASCII codepage
   ✔  And binary data: "\x00\x0F\x00\x00ROUNDTRIP-TEST"
   ✔  When the data is round-tripped
   ✘  Then the round-trip should be lossless
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\rdw_processing.feature:58:5
      Matched: tests\bdd\steps\encode_decode.rs:511:1
      Step panicked. Captured output: Encoded output not set
  Scenario: RDW header validation with non-zero reserved bytes (lenient mode)
   ✔  Given a copybook with content:
   ✔  And RDW record format
   ✔  And ASCII codepage
   ✔  And lenient mode
   ✔  And binary data: "\x00\x05\x12\x34HELLO"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
   ✔  And decoded field VALIDATION-RECORD should be "HELLO"
  Scenario: RDW header validation with non-zero reserved bytes (strict mode)
   ✔  Given a copybook with content:
   ✔  And RDW record format
   ✔  And ASCII codepage
   ✔  And strict mode
   ✔  And binary data: "\x00\x05\x12\x34HELLO"
   ✔  When the binary data is decoded
   ✔  Then decoding should fail
   ✔  And error should contain "RDW"
  Scenario: RDW header validation rejects ASCII-corrupted length bytes
   ✔  Given a copybook with content:
   ✔  And RDW record format
   ✔  And ASCII codepage
   ✔  And binary data: "12\x00\x00HELLO"
   ✔  When the binary data is decoded
   ✔  Then decoding should fail
   ✔  And error should contain "ASCII-corrupted"
  Scenario: Multiple records with RDW processing
   ✔  Given a copybook with content:
   ✔  And RDW record format
   ✔  And ASCII codepage
   ✔  And binary data: "\x00\x08\x00\x00RECORD01\x00\x08\x00\x00RECORD02\x00\x08\x00\x00RECORD03"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
   ✔  And 3 records should be processed
  Scenario: RDW with different record lengths
   ✔  Given a copybook with content:
   ✔  And RDW record format
   ✔  And ASCII codepage
   ✔  And binary data: "\x00\x05\x00\x00SHORT\x00\x10\x00\x00LONGER-DATA-HERE\x00\x0A\x00\x00MEDIUM-LEN"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
   ✘  And 3 records should be processed
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\rdw_processing.feature:126:5
      Matched: tests\bdd\steps\encode_decode.rs:605:1
      Step panicked. Captured output: assertion `left == right` failed: Expected 3 records, got 0
        left: 0
       right: 3
  Scenario: RDW integration with copybook parsing
   ✔  Given a copybook with numeric fields
   ✔  And RDW record format
   ✔  And ASCII codepage
   ?  And binary data: "\x00\x14\x00\x000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
      Step skipped: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\rdw_processing.feature:132:5
Feature: Record I/O Dispatch Microcrate
  Scenario: Fixed encode output round-trips through record I/O dispatch
   ✔  Given a copybook with content:
   ✔  And fixed record format
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should round-trip through the record I/O microcrate in fixed mode with LRECL 8
  Scenario: RDW encode output round-trips through record I/O dispatch
   ✔  Given a copybook with content:
   ✔  And RDW record format
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded output should round-trip through the record I/O microcrate in RDW mode
Feature: Safe operations contract
  Scenario: parse a valid unsigned integer
   ✔  Given the safe-op input is "2048"
   ✔  When safe_ops parses the input as usize
   ✔  Then the parse result should be 2048
  Scenario: parse an invalid unsigned integer
   ✔  Given the safe-op input is "not-a-number"
   ✔  When safe_ops parses the input as usize
   ✔  Then safe_ops should report a syntax error
  Scenario: compute checked array bounds
   ✔  When safe_array_bound is called with base 10, count 3, and item size 4
   ✔  Then safe_array_bound should return 22
  Scenario: handle divide-by-zero without panic
   ✔  When safe_divide is called with numerator 10 and denominator 0
   ✔  Then safe_ops should report a syntax error
  Scenario: parse a valid signed integer via safe-text
   ✔  Given the safe-op input is "-42"
   ✔  When safe_text parses the input as isize
   ✔  Then the safe-text isize parse result should be -42
  Scenario: parse a valid u16 via safe-text
   ✔  Given the safe-op input is "65535"
   ✔  When safe_text parses the input as u16
   ✔  Then the safe-text u16 parse result should be 65535
  Scenario: read a string character in safe-text
   ✔  Given the safe-op input is "abc"
   ✔  When safe_text gets character at index 1
   ✔  Then the safe-text character should be "b"
  Scenario: safe-text reports char out-of-range error
   ✔  Given the safe-op input is "abc"
   ✔  When safe_text gets character at index 10
   ✔  Then safe_ops should report a syntax error
  Scenario: read an in-range index via safe-index
   ✔  When safe_index gets element at index 1
   ✔  Then safe_index should return 20
  Scenario: report error for out-of-range safe-index access
   ✔  When safe_index gets element at index 99
   ✔  Then safe_ops should report a syntax error
Feature: SIGN SEPARATE and RENAMES R4-R6 Support
  Scenario: Parse copybook with SIGN SEPARATE LEADING
   ✔  Given a copybook with SIGN SEPARATE LEADING clause
   ✔  When the copybook is parsed
   ✔  Then parsing should succeed
   ✔  And the field should have sign separate information
   ✔  And the sign placement should be LEADING
  Scenario: Parse copybook with SIGN SEPARATE TRAILING
   ✔  Given a copybook with SIGN SEPARATE TRAILING clause
   ✔  When the copybook is parsed
   ✔  Then parsing should succeed
   ✔  And the field should have sign separate information
   ✔  And the sign placement should be TRAILING
  Scenario: Decode field with SIGN SEPARATE LEADING
   ✔  Given a copybook with SIGN SEPARATE LEADING
   ✔  And ASCII codepage
   ✔  And binary data: "-1234"
   ✔  When the binary data is decoded
   ✘  Then decoding should succeed
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\sign_separate_renames.feature:38:5
      Matched: tests\bdd\steps\encode_decode.rs:579:1
      Step panicked. Captured output: Decoding failed with error: CBKF221_RDW_UNDERFLOW: Incomplete record at end of file: expected 6 bytes (record 1, File ends with partial record)
  Scenario: Decode field with SIGN SEPARATE TRAILING
   ✔  Given a copybook with SIGN SEPARATE TRAILING
   ✔  And ASCII codepage
   ✔  And binary data: "1234-"
   ✔  When the binary data is decoded
   ✘  Then decoding should succeed
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\sign_separate_renames.feature:50:5
      Matched: tests\bdd\steps\encode_decode.rs:579:1
      Step panicked. Captured output: Decoding failed with error: CBKF221_RDW_UNDERFLOW: Incomplete record at end of file: expected 6 bytes (record 1, File ends with partial record)
  Scenario: Encode field with SIGN SEPARATE LEADING
   ✔  Given a copybook with SIGN SEPARATE LEADING
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded data should have leading sign
  Scenario: Encode field with SIGN SEPARATE TRAILING
   ✔  Given a copybook with SIGN SEPARATE TRAILING
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
   ✔  And the encoded data should have trailing sign
  Scenario: Round-trip field with SIGN SEPARATE LEADING
   ✔  Given a copybook with SIGN SEPARATE LEADING
   ✔  And ASCII codepage
   ✔  And binary data: "-1234"
   ✔  When the data is round-tripped
   ✘  Then round-trip should be lossless
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\sign_separate_renames.feature:92:5
      Matched: tests\bdd\steps\encode_decode.rs:724:1
      Step panicked. Captured output: Encoded output not set
  Scenario: Round-trip field with SIGN SEPARATE TRAILING
   ✔  Given a copybook with SIGN SEPARATE TRAILING
   ✔  And ASCII codepage
   ✔  And binary data: "1234-"
   ✔  When the data is round-tripped
   ✘  Then round-trip should be lossless
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\sign_separate_renames.feature:104:5
      Matched: tests\bdd\steps\encode_decode.rs:724:1
      Step panicked. Captured output: Encoded output not set
  Scenario: Parse RENAMES R4 - Multiple REDEFINES
   ✔  Given a copybook with RENAMES R4 (multiple REDEFINES)
   ✔  When the copybook is parsed
   ✘  Then parsing should succeed
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\sign_separate_renames.feature:118:5
      Matched: tests\bdd\steps\parsing.rs:149:1
      Step panicked. Captured output: Parsing failed with error: CBKS610_RENAME_MULTIPLE_REDEFINES: RENAMES alias 'ALIAS-FIELD' spans multiple REDEFINES alternatives. Only single-alternative RENAMES is supported.
  Scenario: Parse RENAMES R5 - OCCURS with DEPENDING ON
   ✔  Given a copybook with RENAMES R5 (ODO)
   ✔  When the copybook is parsed
   ✘  Then parsing should succeed
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\sign_separate_renames.feature:131:5
      Matched: tests\bdd\steps\parsing.rs:149:1
      Step panicked. Captured output: Parsing failed with error: CBKS612_RENAME_ODO_NOT_SUPPORTED: RENAMES alias 'ALIAS-FIELD' spans ODO array 'ARRAY-FIELD'. This pattern is not supported.
  Scenario: Parse RENAMES R6 - Level-88 after RENAMES
   ✔  Given a copybook with RENAMES R6 (Level-88 after RENAMES)
   ✔  When the copybook is parsed
   ✘  Then parsing should succeed
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\sign_separate_renames.feature:147:5
      Matched: tests\bdd\steps\parsing.rs:149:1
      Step panicked. Captured output: Parsing failed with error: CBKS610_RENAME_MULTIPLE_REDEFINES: RENAMES alias 'ALIAS-FIELD' spans multiple REDEFINES alternatives. Only single-alternative RENAMES is supported.
  Scenario: Decode RENAMES R4 field
   ✔  Given a copybook with RENAMES R4
   ✔  And ASCII codepage
   ✔  And binary data: "1234567890ABCDEFGHIJ"
   ✔  When the binary data is decoded
   ✘  Then decoding should succeed
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\sign_separate_renames.feature:163:5
      Matched: tests\bdd\steps\encode_decode.rs:579:1
      Step panicked. Captured output: Decoding failed with error: CBKS610_RENAME_MULTIPLE_REDEFINES: RENAMES alias 'ALIAS-FIELD' spans multiple REDEFINES alternatives. Only single-alternative RENAMES is supported.
  Scenario: Encode RENAMES R4 field
   ✔  Given a copybook with RENAMES R4
   ✔  And ASCII codepage
   ✔  And JSON data:
   ✔  When the JSON data is encoded
   ✘  Then encoding should succeed
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\sign_separate_renames.feature:181:5
      Matched: tests\bdd\steps\encode_decode.rs:568:1
      Step panicked. Captured output: Encoding failed with error: CBKS610_RENAME_MULTIPLE_REDEFINES: RENAMES alias 'ALIAS-FIELD' spans multiple REDEFINES alternatives. Only single-alternative RENAMES is supported.
  Scenario: Round-trip RENAMES R4 field
   ✔  Given a copybook with RENAMES R4
   ✔  And ASCII codepage
   ✔  And binary data: "1234567890ABCDEFGHIJ"
   ✔  When the data is round-tripped
   ✘  Then round-trip should be lossless
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\sign_separate_renames.feature:196:5
      Matched: tests\bdd\steps\encode_decode.rs:724:1
      Step panicked. Captured output: Encoded output not set
  Scenario: Decode RENAMES R5 with ODO
   ✔  Given a copybook with RENAMES R5
   ✔  And ASCII codepage
   ✔  And binary data: "003ELEM1ELEM2ELEM3"
   ✔  When the binary data is decoded
   ✘  Then decoding should succeed
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\sign_separate_renames.feature:210:5
      Matched: tests\bdd\steps\encode_decode.rs:579:1
      Step panicked. Captured output: Decoding failed with error: CBKS612_RENAME_ODO_NOT_SUPPORTED: RENAMES alias 'ALIAS-FIELD' spans ODO array 'ARRAY-FIELD'. This pattern is not supported.
  Scenario: Decode RENAMES R6 with Level-88
   ✔  Given a copybook with RENAMES R6
   ✔  And ASCII codepage
   ✔  And binary data: "1234567890ABCDEFGHIJ"
   ✔  When the binary data is decoded
   ✘  Then decoding should succeed
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\sign_separate_renames.feature:228:5
      Matched: tests\bdd\steps\encode_decode.rs:579:1
      Step panicked. Captured output: Decoding failed with error: CBKS610_RENAME_MULTIPLE_REDEFINES: RENAMES alias 'ALIAS-FIELD' spans multiple REDEFINES alternatives. Only single-alternative RENAMES is supported.
  Scenario: Combined SIGN SEPARATE and RENAMES
   ✔  Given a copybook with both SIGN SEPARATE and RENAMES
   ✔  And ASCII codepage
   ✔  And binary data: "-1234ABCD12"
   ✔  When the binary data is decoded
   ✘  Then decoding should succeed
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\sign_separate_renames.feature:243:5
      Matched: tests\bdd\steps\encode_decode.rs:579:1
      Step panicked. Captured output: Decoding failed with error: CBKS610_RENAME_MULTIPLE_REDEFINES: RENAMES alias 'ALIAS-FIELD' spans multiple REDEFINES alternatives. Only single-alternative RENAMES is supported.
  Scenario: Error on invalid SIGN SEPARATE placement
   ✔  Given a copybook with invalid SIGN SEPARATE
   ✔  When the copybook is parsed
   ✔  Then parsing should fail
   ✔  And the error should indicate invalid SIGN SEPARATE placement
  Scenario: Error on invalid RENAMES range
   ✔  Given a copybook with invalid RENAMES range
   ✔  When the copybook is parsed
   ✔  Then parsing should fail
   ✔  And the error should indicate invalid RENAMES range
Feature: Support Command
  Scenario: Query all features in support matrix
   ✔  When the support matrix is queried
   ✔  Then the matrix should include "Level88Conditions" feature
  Scenario: Check Level-88 conditions feature
   ✔  When feature "level-88" is checked
   ✔  Then the feature should have status "Supported"
  Scenario: Check Level-66 RENAMES feature
   ✔  When feature "level-66-renames" is checked
   ✔  Then the feature should have status "Partial"
  Scenario: Check OCCURS DEPENDING ON feature
   ✔  When feature "occurs-depending" is checked
   ✔  Then the feature should have status "Partial"
  Scenario: Check Edited PIC feature
   ✔  When feature "edited-pic" is checked
   ✔  Then the feature should have status "Supported"
  Scenario: Check SIGN SEPARATE feature
   ✔  When feature "sign-separate" is checked
   ✔  Then the feature should have status "Supported"
  Scenario: Matrix includes multiple features
   ✔  When the support matrix is queried
   ✔  Then the matrix should include "Level66Renames" feature
  Scenario: Unknown feature returns error
   ✔  When feature "nonexistent-feature" is checked
   ✔  Then an error should occur
  Scenario: Feature-governance mapping for signed separate
   ✔  When the governance mapping is checked for feature "sign-separate"
   ✔  Then the governance mapping should include feature flag "sign_separate"
  Scenario: Feature-governance mapping for COMP-1/COMP-2
   ✔  When the governance mapping is checked for feature "comp-1-comp-2"
   ✔  Then the governance mapping should include feature flag "comp_1"
   ✔  And the governance mapping should include feature flag "comp_2"
  Scenario: Governance summary is complete
   ✔  When the governance grid summary is checked
   ✔  Then the governance summary should map 7 support entries
   ✔  And the matrix should include "EditedPic" feature
  Scenario: Governance runtime summary reports feature-flag gating state
   ✔  When the support matrix runtime availability is checked
   ✔  Then the command output should report 7 runtime enabled and 0 runtime disabled support entries
  Scenario: Governance runtime summary reflects disabled feature flags
   ✔  When the support matrix runtime availability is checked with sign-separate disabled
   ✔  Then the command output should report 6 runtime enabled and 1 runtime disabled support entries
Feature: Test Projection Minimal
  Scenario: Test simple projection
   ✔> Given ASCII codepage
   ✔  Given a copybook with content:
   ✔  And field selection: "TEST-FIELD"
   ✔  When the copybook is parsed
   ✔  And the schema is projected with selected fields
   ✔  Then the projection should succeed
Feature: Verify Command
  Scenario: Verify valid fixed-length data
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "JOHN DOE  025"
   ✔  When the data is verified
   ✔  Then verification should succeed
   ✔  And the verify report should contain 0 errors
  Scenario: Verify truncated data fails
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "SHORT"
   ✔  When the data is verified
   ✔  Then verification should fail
  Scenario: Verify valid numeric data
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "001234599"
   ✔  When the data is verified
   ✔  Then verification should succeed
  Scenario: Verify with JSON report format
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "HELLO"
   ✔  When the data is verified with JSON report
   ✔  Then verification should succeed
   ✔  And the verify report should be valid JSON
  Scenario: Verify with field projection
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And field selection: "FIELD-A"
   ✔  And binary data: "HELLOWORLD"
   ✔  When the data is verified
   ✔  Then verification should succeed
  Scenario: Verify all-spaces data
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "          "
   ✔  When the data is verified
   ✔  Then verification should succeed
  Scenario: Verify COMP-3 data
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "\x01\x23\x4C"
   ✔  When the data is verified
   ✔  Then verification should succeed
  Scenario: Verify with strict mode
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And strict mode
   ✔  And binary data: "ABCDEFGHIJ"
   ✔  When the data is verified
   ✔  Then verification should succeed
  Scenario: Verify with lenient mode
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And lenient mode
   ✔  And binary data: "ABCDEFGHIJ"
   ✔  When the data is verified
   ✔  Then verification should succeed
  Scenario: Verify empty record
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: ""
   ✔  When the data is verified
   ✔  Then verification should succeed
  Scenario: Verify group-level copybook
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "HELLO123WORLD"
   ✔  When the data is verified
   ✔  Then verification should succeed
  Scenario: Verify with Normative dialect
   ✔  Given a copybook with content:
   ✔  And Normative dialect
   ✔  And ASCII codepage
   ✔  And binary data: "002ITEM1ITEM2"
   ✔  When the copybook is parsed
   ✔  Then parsing should succeed
  Scenario: Verify JSON report contains status
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "HELLO"
   ✔  When the data is verified with JSON report
   ✔  Then the verify report should be valid JSON
  Scenario: Verify binary integer data
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And binary data: "\x00\x2A"
   ✔  When the data is verified
   ✔  Then verification should succeed
Feature: Zoned Encoding Policies
  Scenario: Auto zoned encoding with ASCII
   ✔> Given a copybook with content:
   ✔  Given ASCII codepage
   ✔  And zoned encoding "auto"
   ✔  And binary data: "12345"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: ASCII zoned encoding
   ✔> Given a copybook with content:
   ✔  Given ASCII codepage
   ✔  And zoned encoding "ascii"
   ✔  And binary data: "12345"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: EBCDIC zoned encoding
   ✔> Given a copybook with content:
   ✔  Given codepage "CP037"
   ✔  And zoned encoding "ebcdic"
   ✔  And binary data for all fields
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: Zoned encoding override for encode
   ✔> Given a copybook with content:
   ✔  Given ASCII codepage
   ✔  And zoned encoding override "ascii"
   ✔  And binary data: "12345"
   ✔  When the data is round-tripped
   ✘  Then the round-trip should be lossless
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\zoned_encoding_policies.feature:38:5
      Matched: tests\bdd\steps\encode_decode.rs:511:1
      Step panicked. Captured output: assertion `left == right` failed: Round-trip should be lossless: original data differs from encoded data
        left: [49, 50, 51, 52, 53]
       right: [49, 50, 51, 52, 69]
  Scenario: Preserve zoned encoding enabled
   ✔> Given a copybook with content:
   ✔  Given ASCII codepage
   ✔  And preserve zoned encoding enabled
   ✔  And binary data: "12345"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: Preferred zoned encoding ASCII
   ✔> Given a copybook with content:
   ✔  Given ASCII codepage
   ✔  And preferred zoned encoding "ascii"
   ✔  And JSON data: "{\"ZONED-FIELD\":\"12345\"}"
   ✔  When the JSON data is encoded
   ✔  Then encoding should succeed
  Scenario: Preferred zoned encoding EBCDIC
   ✔> Given a copybook with content:
   ✔  Given codepage "CP037"
   ✔  And preferred zoned encoding "ebcdic"
   ✔  And binary data for all fields
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: Auto encoding with round-trip
   ✔> Given a copybook with content:
   ✔  Given ASCII codepage
   ✔  And zoned encoding "auto"
   ✔  And binary data: "12345"
   ✔  When the data is round-tripped
   ✘  Then the round-trip should be lossless
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\zoned_encoding_policies.feature:66:5
      Matched: tests\bdd\steps\encode_decode.rs:511:1
      Step panicked. Captured output: assertion `left == right` failed: Round-trip should be lossless: original data differs from encoded data
        left: [49, 50, 51, 52, 53]
       right: [49, 50, 51, 52, 69]
  Scenario: Override none uses default
   ✔> Given a copybook with content:
   ✔  Given ASCII codepage
   ✔  And zoned encoding override "none"
   ✔  And binary data: "12345"
   ✔  When the data is round-tripped
   ✘  Then the round-trip should be lossless
      Step failed:
      Defined: ?\H:\Code\Rust\copybook-rs\tests\bdd\features\zoned_encoding_policies.feature:73:5
      Matched: tests\bdd\steps\encode_decode.rs:511:1
      Step panicked. Captured output: assertion `left == right` failed: Round-trip should be lossless: original data differs from encoded data
        left: [49, 50, 51, 52, 53]
       right: [49, 50, 51, 52, 69]
  Scenario: Zoned encoding with decimal field
   ✔> Given a copybook with content:
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And zoned encoding "auto"
   ✔  And binary data: "1234567"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: Zoned encoding with unsigned field
   ✔> Given a copybook with content:
   ✔  Given a copybook with content:
   ✔  And ASCII codepage
   ✔  And zoned encoding "ascii"
   ✔  And binary data: "12345"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
  Scenario: Zoned encoding output is valid JSON
   ✔> Given a copybook with content:
   ✔  Given ASCII codepage
   ✔  And zoned encoding "auto"
   ✔  And binary data: "00042"
   ✔  When the binary data is decoded
   ✔  Then decoding should succeed
   ✔  And the decoded output should be valid JSON
[Summary]
37 features
497 scenarios (380 passed, 37 skipped, 80 failed)
2794 steps (2677 passed, 37 skipped, 80 failed)
